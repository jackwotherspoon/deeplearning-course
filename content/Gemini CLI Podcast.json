{"text": " This week, on The Agent Factory, it's the agentic looping action. I'm not very good at like sitting down and reading. My re-later folder has its own gravitational pull. With AI, using AI these days, it is so easy to 10x to yourself. 100x and that's the hard part. Yeah, and my mom watches these, so mom, I'm still alive. Hi, everyone, and welcome to The Agent Factory, the podcast that goes beyond the hype, and dives into building production-ready AI agents. I'm Molly Bennett. And I'm Emma Morage. And today, we're going to be diving deep into the Gemini CLI. So we used the CLI a bit in our last episode, including this live vibe coding demo that was a ton of fun. I suggest you check it out if you haven't seen it yet. I'm really excited to show it off in more detail. And for anyone who hasn't heard or haven't, hasn't heard of or used the Gemini CLI yet, it's a really powerful agent, lives right in your command line, and it's designed to help you with your everyday workflows. Yeah, and I'm really excited because in this episode, we're actually going to show you how to use Gemini CLI or getting up to speed on a new code base, super charging your research, and how you can use it to integrate it into your own automations. Yeah, and to help us do that, we're very excited to have Taylor Mullin, the creator of Gemini CLI with us today. We're going to pick his brain about the philosophy behind the tool, and what's next on the roadmap, which I'm very excited to hear about. Yeah, I can't wait. I mean, I've been using the CLI now for a few weeks, and I already feel like it's become a mainstay in my workflow. What about you, Molly, what's your take? Oh, yeah, no, I'm totally sold. For me, the biggest thing is that it lives in the terminal, where I do my work, and I was also able to add a directly to VS code, the IDE I use, so that was super handy. Yeah, I feel like having it integrated into so many different tools makes it indispensable. And also on that note, having Taylor with us here today, who's going to be able to tell us even more about the background in philosophy, behind Gemini CLI, will be invaluable. But before we talk to Taylor, let's get hands-on with some demos on the facts before. Let's tackle a universal developer problem, which is onboarding onto a new code base. I think all of us have had to do this at some point. I'm going to go ahead and show how the Gemini CLI can help speed up this onboarding process. Okay, yeah, so code base exploration, that sounds pretty cool. And I feel like this will also be a really good showcase for Gemini CLI's massive 1 million token context window. I feel like that's kind of the difference between asking questions about a single file versus maybe the entire architecture all at one. Yeah, yeah, totally. And so to start, I'm actually not even going to clone the repo myself. I'm just going to ask the agent to do it for me. So I've decided to use Google's ADK repo for this example. So ADK stands for agent development kit. So I'm just going to say clone the Google ADK Python repo from GitHub. And I'm going to see what it does. Okay, so it's searching the web trying to find the right link. Awesome. It found the right link. It's asking me for permission to clone it. Go ahead. Cool. Yeah, and this looks like it's the agent group in action. It's kind of reasoning that it needs a piece of information. It then chooses a tool to get that information looks like. And then it uses that information to complete the original request. And then also at the same time, it asks you if you want to proceed. Checking back in. So yeah, this is a super helpful workflow. I think the way I want to tackle this is let's imagine that we're like a new contributor to this project. What do you think is the first like what's the first thing you'd want to do if you were a new contributor to a project and you wanted to figure out how to go forward. Feel like I would want to get a high level understanding of the project. I think I learned best maybe from like a top down perspective. Things like text stack, how code of structure, stuff like that. Yeah, yeah, totally like in the past. And then not so distant past actually. You know, you might start with like reading a read me file. Maybe contributing file. You know, start piecing things together that way. And like. I'm not very good at like sitting down and reading a huge amount of documentation to be quite honest. So instead, I'm going to ask the Gemini CLI to perform a high level audit of the entire directory. So let's see. I'm going to ask the Gemini CLI agent for a complete project overview. I want it to tell me the purpose, the text stack and analyze architecture all in one go. Yeah, and I feel like that's much more powerful than just relying on one read me document. This way you can kind of point at the whole directory. And it's cool because you're asking it to like synthesize everything. The source, file, the docs, all of the code into a single coherent sort of overview, right? Yeah. Yeah, absolutely. Yeah, it's like it's ingesting the entire directory to build out this like complete mental model of the project, which is super helpful. Okay, great. Starting to output this looks really good and helpful. Yeah, identified the purpose, the text stack, core dependencies, all that good stuff. That's like helpful to kind of know what the outlet. Yeah, it's really verbose what it did. Summary, I wouldn't have been able to write it better myself, right? Yeah. And now for the part of the show that we've both been waiting for. We're going to shift gears and get a deep dive into the philosophy and future of Gemini CLI. We'd now like to welcome to the podcast Taylor Mullin, creator of Gemini CLI. Taylor, thank you so much for joining us. Thanks for having me. This is fun to be here. Yeah, so let's start at the very beginning. Could you tell us the origin story of the Gemini CLI? Like, what was the initial spark or inspiration that led to it? And how did things go from there? I'm very curious to hear. Oh, yeah, it's kind of crazy. It started about a year and a half ago, weirdly enough, right? It's so long ago. But like, at the time, I was experimenting with multi agent systems and trying to get like the most out of our elements. And so at the time, like, I had the system where the multiple personas, they were talking to each other and they were doing a lot of cool things. And it was surfaced in a lot of different ways. It was surfaced in IDE, it was surfaced in a CLI. It was surfaced in the web. And as I was playing with these things, the thing that really stuck for me was the CLI. It was so easy to use, so lightweight. But at the time, like, it took 30 requests, like one user request resulted in like 30 LLM requests behind the scenes. And those 30 requests was a lot for the time. It took a minute and a half to respond to anything. And the quality was there. Like, this was something at the time that was just too much. Like, we also have limited context window. And so in that realm, it being a CLI and it having all these constraints, we scrapped the project. And that was kind of challenging at the time. It was almost a little bit too early. Yeah. It was one of those things like, okay, no one's going to want to use this because it cost too much. It couldn't absorb enough information. And it took too long to respond, which is kind of a trippy thing like fast forwarding today, right? We're very much used to, you have deep research, like you click a button. And then you go off and you get a coffee and you come back and hopefully it's done. And it's crazy. It's so different. But during that time, like, it was too early. And then today, like me, Google and me, I'm really trying to like build this future of developer tools. Like, there was this ecosystem of CLIs that was just taking hold in the community, right? And it really proved that people are willing to wait. People are willing to use a CLI. And they find it, they found it as compelling as they I did back then. It's like, when all this started to happen, I'm like, oh my goodness. Maybe I was just too early and maybe we should try and give this another go. And so that like, I really grounded at least the motivation of saying, okay, I've done this before. And like, I've learned a lot over my entire career of what it means to build developer tools and what it means. It's like, build something that can kind of span a lot of ecosystems. And so we started Gemini CLI from that point. So it's like, I basically, I went dark, obviously, I went dark for like a week. And I'm like, okay, I'm going to do this. I have this idea that went dark for a week, talked to no one, spent every waking second, was getting like five to six hours of sleep in the night. And this was like, I think it was like Saturday through Saturday. Saturday through Sunday or something like that. And then like that Monday, I had a prototype, did a video. I put it out and Googlers started just going like wild for it. It was super cool. Yeah, thanks so much for sharing that. And so I guess building on that, you know, making the Gemini CLI open source was a very deliberate choice, I think, which you've linked to security and learning and a whole host of other things. So how does this open, trust-based approach contrast with the more like black box nature of other maybe major AI tools out there? And what kind of ecosystem do you hope that this will foster the transparency will foster? Yeah, it's like to be frigate. I started my career in open source. Like I have always been in developer tooling or framework to one thing and the other and open source has been like front and center. So like from day zero, that was a key thing in my head. But most folks feel like, oh, it's open source. Why would you not go ahead and do it? Open source is not free. It's not free. It's actually a very challenging thing to get right. But oh my gosh, it's so rewarding when you do. So like knowing this, it was always this question like, okay, let's weigh the actual balance. Like should it be open source should not be in the answers ended being like obviously it should be open source, like especially with a CLI tool like this. That's forward like for a wide variety of people and of course developers as well. But something that people could see and could understand, okay, it's running on my box. It has a lot of capability. So what does it do and can I trust it? It's like that kind of like you mentioned the security aspect. That was one of our biggest reasons for like open sourcing this. We want people to see exactly how it operates. So they can have they can have trust. They know we're not doing anything behind the scenes. They know exactly what's happening. And also means that if we make a mistake. That we can fix it that we have the entire community to help keep us grounded. And what makes the most secure sense we to be frank, we struggle to keep up. We totally struggle to keep up with all the energy they give. But to be frank, it's one of the most important things that we have. And so when people ask me like, what is the number one thing that's on your mind for Jim and I see a line like it's our open source community. Like by far, it's number one. And that's like, I think we've shut down the team. Like literally everyone on the team for like days in order to make sure that we can try and keep up with the amount of traction and the amount of energy coming at us. But I think that's because we see the value. We see how valuable it is to kind of build this in the open together. So that folks have that confidence that we're doing the right thing. And we have that confidence we're building the right things as well. And it's funny. We actually still do the updates today. Actually on all of our socials will post every single week on Wednesdays will post like 100 to 150 features weekly features bugs enhancements all the above. You said 150 features a week, right? That's a lot. What are the mechanisms that you use you and the team used to make that possible? Because we use it to build itself like it allows us to. Allow us to do a lot more like one of the coolest things I think that we started doing is when you teach it to boot itself. It means it can spin up parallel like threads simultaneously. And each of those can go ahead and tackle different problems. Combine that with work get work trees and you're going even further so far gone are the days of being able to ship like twice a year. Which is a huge part of software historically. Because in AI every single week is like an insane amount of time. Every single month is like yeah. Absolutely. So I think it's it's honestly it's just that it's a mindset such cultural thing that we've built. And I also want to kind of acknowledge our open source community again as well. Which is they really help make it possible because without folks really contributing helping each other out even. I don't know how easy it would it would be like we put human eyes on every single one of the changes that go and we don't we're not just like letting things go without looking at them. It's just we have a lot of really passionate really smart capable and motivated people to kind of make the right decisions at scale. You use Gemini CLI to build Gemini CLI so I'm curious to tell a little bit more about how that journey started and and kind of what was your favorite part of it. Yeah, it's kind of nuts like I still remember actually the first feature it built for itself. Which is it's kind of crazy it was I see it's so long but it really wasn't that long ago. But early on we knew that markdown rendering was going to be important right. So I'm sitting there and I'm trying to build out its ability to render mark down a set of seeing the raw markdowns in the terminal. And there was a lot of utilities and frameworks to make this possible and I kept I forget the exact of the wall that I was hitting but I kept hitting a wall and using one of these frameworks that was like just a limiter and this was a hard stop for me. And okay, like I need to progress I need to do something and I ended up asking hey like what are my options as the first I'm just asking is that when I was asking it more questions to tell learn more. And it's like oh I can write a markdown parser for you. I said cool go for it. And so this is the first time when it one shot its own markdown rendering. And to be frank the variant of that is still used today that actually renders. So if you're ever curious like if like Gemini CLI has written a significant amount of its own code. It's super significant amount so cool. Yeah, it's we doubt like I think the biggest thing that we think about on the team is with AI using AI these days it is so easy to 10X yourself which sounds crazy to say but 10Xing is easy these days 100Xing that's the hard part like that is where you really start getting into how can I parallelize my workflows to make sure my time that I'm investing in each of these things is best spent. Because models themselves that we talked about it there like there's so much that's left unsaid when asking a question they it still needs human feedback a lot of the times in order to be effective like things don't just get one shot of the right of course you see that in all the videos out there and like all the highlights. But one shoting rarely happens so how do you optimize your time to make it so that multi shot scenarios are the land super well they can really amplify what you do. Another thing I'm curious about is are there any like methodologies you think about when using Gemini CLI like what's your mentality on how the tool operates is grounded in a lot of my developer background especially with AI. Which is I personally feel like there's so much that's left unsaid when people will do a prop so like one of the big things is this is context engineering it's kind of grounded in the same mindset which is when you ask a question to your AI of choice you're giving it as much information as you possibly can to enable it to derive the right answer. Now of course you're also leaning on its ability to like figure out more information and dig through your co base or whatever you're by doing at the time but that alone is really challenging spot because it can't know those offline conversations you have with your colleague or your friend it can't know. Usually doesn't know your emails and your chat messages but it doesn't typically know that all those pieces of context really feed into building a coherent response so if you're currently writing tests and you ask to go ahead and implement something chances are you want to implement that in the form of tests. Your current workflow that methodology though it kind of rings true with every decision we make I think the thing I tell the team is that do what a person would do and don't take shortcuts so one thing I might be surprising to folks is we don't use embeddings like for instance for search we do not index your code base we do a gentick search which means how you as a person or use as a developer would dig through a piece of code we do the same thing. We'll do fine we'll do we'll we'll grab our way through the code base we'll open files or read them will effectively run commands like find all references on your behalf if we need to to find out what's the next piece of the puzzle because in the end we're trying to provide the right context to the LM so that it's grounded in every single thing it does to come to a good a good result so I think you think it's probably it was probably the biggest one. I think that kind of really resonates with us something that I think is really cool is like maybe I'll ask the CLI to do something and it'll try to do the thing but be like well I can't actually do this. But then you'll give me the steps that it needs to take in order to do it and be like are you okay with me going ahead and doing these steps to do the thing that I need to do to get to your original request which is very cool. I love that's the that's such a huge unlock is the we call it self-healing it's ability to self-heal goes so far and like when it they'll try things like it has a good idea of what's on your box and to try to use all those things that when it can't do it it's so good at coming of other alternatives like I recall this one scenario. I was talking to our marketing folks and I was giving them a demo to show what it could do and then the first question was oh can I like can you give me a link to this and I'm like oh. It's we don't we don't have built in like deploy functionality like there's a cloud run and there's a cloud run extension actually being released very soon that will enable this just the folks know. But we don't have that built in and so they're asking how can you do this I'm like okay well let me just ask. And what it did is it ended up like creating a GitHub repository and GitHub repositories have a thing called GitHub pages which allows you to host static content and then it pushed this content to it and it gave me a link and I gave it I'm like I had never even considered that my question was how do I give this person like it but all the other pieces together to kind of make that a reality. Like the scrap scrapiest developer yeah I'm almost thinking of the use case where I'm like hey Gemini CLI how can I like tell my mom that I'm still alive like up there and it'll like ask me maybe you should do that but hear the steps that I would take like are you okay. I think about the text messages and whatever. Totally yeah and my mom watches these so mom I'm still alive. Well and also a moment ago you mentioned something about like an upcoming feature on the roadmap. So what else like what else is coming up on the roadmap like are there any features coming up that you're most excited for. Because we view Gemini CLI as being a lot more than just developers and because we've seen internally it's unlocking every profession whether you're a marketer or a financial to of course the software developer it kind of tax that hits all those buttons to make it so we can work with every one of these professions. We're really doubling down an extensibility so you can heavily extend Gemini CLI and this is not just MCP servers this is literally you can install an extension which is a bundle of it could be available. MCP servers specific instructions specific commands lots of different things to drive a different so this is the cloud run one that I had mentioned earlier they have an extension. And this extension can be Gemini extensions install and then even passing cloud run and it's seamless installation but enables you to really curate the experience to your like your preferences. So for instance if you are like a go developer and you want to make sure your environment is super go friendly like you'll install the right MCP servers to make that happen right or if you are it's a content generator maybe you'll hook it up to all your various socials maybe you'll create a generative media APIs you'll also hook that up to it. So being able to turn these on and off is something that's super important to us because we know that there's a lot of use cases and so the big so that's the biggest feature that we're going to be talking about soon which is how to build these extensions how to install them manage them with the intention of making this super seamless for people where people can spin up their own registries for they want to we're going to have eventually going to have like a centralized registry for all of our extensions. But the extension ecosystem is going to be the big one I can't wait to see what people build like we have a number of them coming out from Google from from cloud. And just in general to really hook Gemini CLI into everything in a really seamless way. Yeah Taylor I just wanted to thank you so much for sharing your insights with us in our audience of agent builders it's been a fascinating look behind the curtain of Gemini CLI and you've shared so much with us so so I want to thank you. Oh thanks for having me this has been an amazing conversation like I love I love diving in and especially love just sharing stories and if you haven't checked this out checks out on GitHub you'll find us all the amazing like Gemini Google dash Gemini slash Gemini CLI and get up and then of course you can look for us in socials as well to get those weekly updates that we push out regularly. Oh yeah good plug yeah thank you so much Taylor this is so fun. Yeah thanks so much Taylor. And that's our show for today thank you for joining us for this deep dive into the Gemini CLI we highly recommend you try it out for yourself. And if you enjoyed this episode of the agent factory check us out next time where we'll continue diving into the world of AI agents until then I'm Emmett Mirage and I'm Molly Pettit. Power it down.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.8, "text": " This week, on The Agent Factory, it's the agentic looping action.", "tokens": [50364, 639, 1243, 11, 322, 440, 27174, 36868, 11, 309, 311, 264, 623, 317, 299, 6367, 278, 3069, 13, 50654], "temperature": 0.0, "avg_logprob": -0.39486867731267755, "compression_ratio": 1.461187214611872, "no_speech_prob": 0.13815435767173767}, {"id": 1, "seek": 0, "start": 5.8, "end": 8.0, "text": " I'm not very good at like sitting down and reading.", "tokens": [50654, 286, 478, 406, 588, 665, 412, 411, 3798, 760, 293, 3760, 13, 50764], "temperature": 0.0, "avg_logprob": -0.39486867731267755, "compression_ratio": 1.461187214611872, "no_speech_prob": 0.13815435767173767}, {"id": 2, "seek": 0, "start": 8.0, "end": 11.8, "text": " My re-later folder has its own gravitational pull.", "tokens": [50764, 1222, 319, 12, 75, 771, 10820, 575, 1080, 1065, 28538, 2235, 13, 50954], "temperature": 0.0, "avg_logprob": -0.39486867731267755, "compression_ratio": 1.461187214611872, "no_speech_prob": 0.13815435767173767}, {"id": 3, "seek": 0, "start": 11.8, "end": 16.0, "text": " With AI, using AI these days, it is so easy to 10x to yourself.", "tokens": [50954, 2022, 7318, 11, 1228, 7318, 613, 1708, 11, 309, 307, 370, 1858, 281, 1266, 87, 281, 1803, 13, 51164], "temperature": 0.0, "avg_logprob": -0.39486867731267755, "compression_ratio": 1.461187214611872, "no_speech_prob": 0.13815435767173767}, {"id": 4, "seek": 0, "start": 16.0, "end": 19.0, "text": " 100x and that's the hard part.", "tokens": [51164, 2319, 87, 293, 300, 311, 264, 1152, 644, 13, 51314], "temperature": 0.0, "avg_logprob": -0.39486867731267755, "compression_ratio": 1.461187214611872, "no_speech_prob": 0.13815435767173767}, {"id": 5, "seek": 0, "start": 19.0, "end": 22.0, "text": " Yeah, and my mom watches these, so mom, I'm still alive.", "tokens": [51314, 865, 11, 293, 452, 1225, 17062, 613, 11, 370, 1225, 11, 286, 478, 920, 5465, 13, 51464], "temperature": 0.0, "avg_logprob": -0.39486867731267755, "compression_ratio": 1.461187214611872, "no_speech_prob": 0.13815435767173767}, {"id": 6, "seek": 2200, "start": 22.0, "end": 32.0, "text": " Hi, everyone, and welcome to The Agent Factory, the podcast that goes beyond the hype,", "tokens": [50364, 2421, 11, 1518, 11, 293, 2928, 281, 440, 27174, 36868, 11, 264, 7367, 300, 1709, 4399, 264, 24144, 11, 50864], "temperature": 0.0, "avg_logprob": -0.19685354327211285, "compression_ratio": 1.5109170305676856, "no_speech_prob": 0.009940605610609055}, {"id": 7, "seek": 2200, "start": 32.0, "end": 35.0, "text": " and dives into building production-ready AI agents.", "tokens": [50864, 293, 274, 1539, 666, 2390, 4265, 12, 1201, 7318, 12554, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19685354327211285, "compression_ratio": 1.5109170305676856, "no_speech_prob": 0.009940605610609055}, {"id": 8, "seek": 2200, "start": 35.0, "end": 37.0, "text": " I'm Molly Bennett.", "tokens": [51014, 286, 478, 26665, 40620, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19685354327211285, "compression_ratio": 1.5109170305676856, "no_speech_prob": 0.009940605610609055}, {"id": 9, "seek": 2200, "start": 37.0, "end": 38.0, "text": " And I'm Emma Morage.", "tokens": [51114, 400, 286, 478, 17124, 5146, 609, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19685354327211285, "compression_ratio": 1.5109170305676856, "no_speech_prob": 0.009940605610609055}, {"id": 10, "seek": 2200, "start": 38.0, "end": 42.0, "text": " And today, we're going to be diving deep into the Gemini CLI.", "tokens": [51164, 400, 965, 11, 321, 434, 516, 281, 312, 20241, 2452, 666, 264, 22894, 3812, 12855, 40, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19685354327211285, "compression_ratio": 1.5109170305676856, "no_speech_prob": 0.009940605610609055}, {"id": 11, "seek": 2200, "start": 42.0, "end": 48.0, "text": " So we used the CLI a bit in our last episode, including this live vibe coding demo", "tokens": [51364, 407, 321, 1143, 264, 12855, 40, 257, 857, 294, 527, 1036, 3500, 11, 3009, 341, 1621, 14606, 17720, 10723, 51664], "temperature": 0.0, "avg_logprob": -0.19685354327211285, "compression_ratio": 1.5109170305676856, "no_speech_prob": 0.009940605610609055}, {"id": 12, "seek": 2200, "start": 48.0, "end": 49.0, "text": " that was a ton of fun.", "tokens": [51664, 300, 390, 257, 2952, 295, 1019, 13, 51714], "temperature": 0.0, "avg_logprob": -0.19685354327211285, "compression_ratio": 1.5109170305676856, "no_speech_prob": 0.009940605610609055}, {"id": 13, "seek": 4900, "start": 49.0, "end": 52.0, "text": " I suggest you check it out if you haven't seen it yet.", "tokens": [50364, 286, 3402, 291, 1520, 309, 484, 498, 291, 2378, 380, 1612, 309, 1939, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10200381482768263, "compression_ratio": 1.6932270916334662, "no_speech_prob": 0.009064529091119766}, {"id": 14, "seek": 4900, "start": 52.0, "end": 55.0, "text": " I'm really excited to show it off in more detail.", "tokens": [50514, 286, 478, 534, 2919, 281, 855, 309, 766, 294, 544, 2607, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10200381482768263, "compression_ratio": 1.6932270916334662, "no_speech_prob": 0.009064529091119766}, {"id": 15, "seek": 4900, "start": 55.0, "end": 61.0, "text": " And for anyone who hasn't heard or haven't, hasn't heard of or used the Gemini CLI yet,", "tokens": [50664, 400, 337, 2878, 567, 6132, 380, 2198, 420, 2378, 380, 11, 6132, 380, 2198, 295, 420, 1143, 264, 22894, 3812, 12855, 40, 1939, 11, 50964], "temperature": 0.0, "avg_logprob": -0.10200381482768263, "compression_ratio": 1.6932270916334662, "no_speech_prob": 0.009064529091119766}, {"id": 16, "seek": 4900, "start": 61.0, "end": 65.0, "text": " it's a really powerful agent, lives right in your command line,", "tokens": [50964, 309, 311, 257, 534, 4005, 623, 317, 11, 2909, 558, 294, 428, 5622, 1622, 11, 51164], "temperature": 0.0, "avg_logprob": -0.10200381482768263, "compression_ratio": 1.6932270916334662, "no_speech_prob": 0.009064529091119766}, {"id": 17, "seek": 4900, "start": 65.0, "end": 69.0, "text": " and it's designed to help you with your everyday workflows.", "tokens": [51164, 293, 309, 311, 4761, 281, 854, 291, 365, 428, 7429, 43461, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10200381482768263, "compression_ratio": 1.6932270916334662, "no_speech_prob": 0.009064529091119766}, {"id": 18, "seek": 4900, "start": 69.0, "end": 74.0, "text": " Yeah, and I'm really excited because in this episode, we're actually going to show you how to use Gemini CLI", "tokens": [51364, 865, 11, 293, 286, 478, 534, 2919, 570, 294, 341, 3500, 11, 321, 434, 767, 516, 281, 855, 291, 577, 281, 764, 22894, 3812, 12855, 40, 51614], "temperature": 0.0, "avg_logprob": -0.10200381482768263, "compression_ratio": 1.6932270916334662, "no_speech_prob": 0.009064529091119766}, {"id": 19, "seek": 7400, "start": 74.0, "end": 78.0, "text": " or getting up to speed on a new code base, super charging your research,", "tokens": [50364, 420, 1242, 493, 281, 3073, 322, 257, 777, 3089, 3096, 11, 1687, 11379, 428, 2132, 11, 50564], "temperature": 0.0, "avg_logprob": -0.0717010624361354, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.10629503428936005}, {"id": 20, "seek": 7400, "start": 78.0, "end": 82.0, "text": " and how you can use it to integrate it into your own automations.", "tokens": [50564, 293, 577, 291, 393, 764, 309, 281, 13365, 309, 666, 428, 1065, 3553, 763, 13, 50764], "temperature": 0.0, "avg_logprob": -0.0717010624361354, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.10629503428936005}, {"id": 21, "seek": 7400, "start": 82.0, "end": 86.0, "text": " Yeah, and to help us do that, we're very excited to have Taylor Mullin,", "tokens": [50764, 865, 11, 293, 281, 854, 505, 360, 300, 11, 321, 434, 588, 2919, 281, 362, 12060, 41621, 259, 11, 50964], "temperature": 0.0, "avg_logprob": -0.0717010624361354, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.10629503428936005}, {"id": 22, "seek": 7400, "start": 86.0, "end": 89.0, "text": " the creator of Gemini CLI with us today.", "tokens": [50964, 264, 14181, 295, 22894, 3812, 12855, 40, 365, 505, 965, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0717010624361354, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.10629503428936005}, {"id": 23, "seek": 7400, "start": 89.0, "end": 93.0, "text": " We're going to pick his brain about the philosophy behind the tool,", "tokens": [51114, 492, 434, 516, 281, 1888, 702, 3567, 466, 264, 10675, 2261, 264, 2290, 11, 51314], "temperature": 0.0, "avg_logprob": -0.0717010624361354, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.10629503428936005}, {"id": 24, "seek": 7400, "start": 93.0, "end": 96.0, "text": " and what's next on the roadmap, which I'm very excited to hear about.", "tokens": [51314, 293, 437, 311, 958, 322, 264, 35738, 11, 597, 286, 478, 588, 2919, 281, 1568, 466, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0717010624361354, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.10629503428936005}, {"id": 25, "seek": 7400, "start": 96.0, "end": 97.0, "text": " Yeah, I can't wait.", "tokens": [51464, 865, 11, 286, 393, 380, 1699, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0717010624361354, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.10629503428936005}, {"id": 26, "seek": 7400, "start": 97.0, "end": 100.0, "text": " I mean, I've been using the CLI now for a few weeks,", "tokens": [51514, 286, 914, 11, 286, 600, 668, 1228, 264, 12855, 40, 586, 337, 257, 1326, 3259, 11, 51664], "temperature": 0.0, "avg_logprob": -0.0717010624361354, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.10629503428936005}, {"id": 27, "seek": 7400, "start": 100.0, "end": 103.0, "text": " and I already feel like it's become a mainstay in my workflow.", "tokens": [51664, 293, 286, 1217, 841, 411, 309, 311, 1813, 257, 2135, 372, 320, 294, 452, 20993, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0717010624361354, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.10629503428936005}, {"id": 28, "seek": 10300, "start": 103.0, "end": 105.0, "text": " What about you, Molly, what's your take?", "tokens": [50364, 708, 466, 291, 11, 26665, 11, 437, 311, 428, 747, 30, 50464], "temperature": 0.0, "avg_logprob": -0.1017464444153291, "compression_ratio": 1.580536912751678, "no_speech_prob": 0.005114553030580282}, {"id": 29, "seek": 10300, "start": 105.0, "end": 107.0, "text": " Oh, yeah, no, I'm totally sold.", "tokens": [50464, 876, 11, 1338, 11, 572, 11, 286, 478, 3879, 3718, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1017464444153291, "compression_ratio": 1.580536912751678, "no_speech_prob": 0.005114553030580282}, {"id": 30, "seek": 10300, "start": 107.0, "end": 111.0, "text": " For me, the biggest thing is that it lives in the terminal,", "tokens": [50564, 1171, 385, 11, 264, 3880, 551, 307, 300, 309, 2909, 294, 264, 14709, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1017464444153291, "compression_ratio": 1.580536912751678, "no_speech_prob": 0.005114553030580282}, {"id": 31, "seek": 10300, "start": 111.0, "end": 115.0, "text": " where I do my work, and I was also able to add a directly to VS code,", "tokens": [50764, 689, 286, 360, 452, 589, 11, 293, 286, 390, 611, 1075, 281, 909, 257, 3838, 281, 25091, 3089, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1017464444153291, "compression_ratio": 1.580536912751678, "no_speech_prob": 0.005114553030580282}, {"id": 32, "seek": 10300, "start": 115.0, "end": 118.0, "text": " the IDE I use, so that was super handy.", "tokens": [50964, 264, 40930, 286, 764, 11, 370, 300, 390, 1687, 13239, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1017464444153291, "compression_ratio": 1.580536912751678, "no_speech_prob": 0.005114553030580282}, {"id": 33, "seek": 10300, "start": 118.0, "end": 121.0, "text": " Yeah, I feel like having it integrated into so many different tools", "tokens": [51114, 865, 11, 286, 841, 411, 1419, 309, 10919, 666, 370, 867, 819, 3873, 51264], "temperature": 0.0, "avg_logprob": -0.1017464444153291, "compression_ratio": 1.580536912751678, "no_speech_prob": 0.005114553030580282}, {"id": 34, "seek": 10300, "start": 121.0, "end": 123.0, "text": " makes it indispensable.", "tokens": [51264, 1669, 309, 47940, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1017464444153291, "compression_ratio": 1.580536912751678, "no_speech_prob": 0.005114553030580282}, {"id": 35, "seek": 10300, "start": 123.0, "end": 126.0, "text": " And also on that note, having Taylor with us here today,", "tokens": [51364, 400, 611, 322, 300, 3637, 11, 1419, 12060, 365, 505, 510, 965, 11, 51514], "temperature": 0.0, "avg_logprob": -0.1017464444153291, "compression_ratio": 1.580536912751678, "no_speech_prob": 0.005114553030580282}, {"id": 36, "seek": 10300, "start": 126.0, "end": 130.0, "text": " who's going to be able to tell us even more about the background in philosophy,", "tokens": [51514, 567, 311, 516, 281, 312, 1075, 281, 980, 505, 754, 544, 466, 264, 3678, 294, 10675, 11, 51714], "temperature": 0.0, "avg_logprob": -0.1017464444153291, "compression_ratio": 1.580536912751678, "no_speech_prob": 0.005114553030580282}, {"id": 37, "seek": 13000, "start": 130.0, "end": 133.0, "text": " behind Gemini CLI, will be invaluable.", "tokens": [50364, 2261, 22894, 3812, 12855, 40, 11, 486, 312, 40367, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10121086120605469, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.0025747795589268208}, {"id": 38, "seek": 13000, "start": 133.0, "end": 139.0, "text": " But before we talk to Taylor, let's get hands-on with some demos on the facts before.", "tokens": [50514, 583, 949, 321, 751, 281, 12060, 11, 718, 311, 483, 2377, 12, 266, 365, 512, 33788, 322, 264, 9130, 949, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10121086120605469, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.0025747795589268208}, {"id": 39, "seek": 13000, "start": 139.0, "end": 145.0, "text": " Let's tackle a universal developer problem,", "tokens": [50814, 961, 311, 14896, 257, 11455, 10754, 1154, 11, 51114], "temperature": 0.0, "avg_logprob": -0.10121086120605469, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.0025747795589268208}, {"id": 40, "seek": 13000, "start": 145.0, "end": 148.0, "text": " which is onboarding onto a new code base.", "tokens": [51114, 597, 307, 24033, 278, 3911, 257, 777, 3089, 3096, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10121086120605469, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.0025747795589268208}, {"id": 41, "seek": 13000, "start": 148.0, "end": 150.0, "text": " I think all of us have had to do this at some point.", "tokens": [51264, 286, 519, 439, 295, 505, 362, 632, 281, 360, 341, 412, 512, 935, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10121086120605469, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.0025747795589268208}, {"id": 42, "seek": 13000, "start": 150.0, "end": 156.0, "text": " I'm going to go ahead and show how the Gemini CLI can help speed up this onboarding process.", "tokens": [51364, 286, 478, 516, 281, 352, 2286, 293, 855, 577, 264, 22894, 3812, 12855, 40, 393, 854, 3073, 493, 341, 24033, 278, 1399, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10121086120605469, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.0025747795589268208}, {"id": 43, "seek": 15600, "start": 157.0, "end": 161.0, "text": " Okay, yeah, so code base exploration, that sounds pretty cool.", "tokens": [50414, 1033, 11, 1338, 11, 370, 3089, 3096, 16197, 11, 300, 3263, 1238, 1627, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0845030055326574, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.031632933765649796}, {"id": 44, "seek": 15600, "start": 161.0, "end": 165.0, "text": " And I feel like this will also be a really good showcase for Gemini CLI's massive", "tokens": [50614, 400, 286, 841, 411, 341, 486, 611, 312, 257, 534, 665, 20388, 337, 22894, 3812, 12855, 40, 311, 5994, 50814], "temperature": 0.0, "avg_logprob": -0.0845030055326574, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.031632933765649796}, {"id": 45, "seek": 15600, "start": 165.0, "end": 168.0, "text": " 1 million token context window.", "tokens": [50814, 502, 2459, 14862, 4319, 4910, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0845030055326574, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.031632933765649796}, {"id": 46, "seek": 15600, "start": 168.0, "end": 172.0, "text": " I feel like that's kind of the difference between asking questions about a single file", "tokens": [50964, 286, 841, 411, 300, 311, 733, 295, 264, 2649, 1296, 3365, 1651, 466, 257, 2167, 3991, 51164], "temperature": 0.0, "avg_logprob": -0.0845030055326574, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.031632933765649796}, {"id": 47, "seek": 15600, "start": 172.0, "end": 175.0, "text": " versus maybe the entire architecture all at one.", "tokens": [51164, 5717, 1310, 264, 2302, 9482, 439, 412, 472, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0845030055326574, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.031632933765649796}, {"id": 48, "seek": 15600, "start": 175.0, "end": 177.0, "text": " Yeah, yeah, totally.", "tokens": [51314, 865, 11, 1338, 11, 3879, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0845030055326574, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.031632933765649796}, {"id": 49, "seek": 15600, "start": 177.0, "end": 182.0, "text": " And so to start, I'm actually not even going to clone the repo myself.", "tokens": [51414, 400, 370, 281, 722, 11, 286, 478, 767, 406, 754, 516, 281, 26506, 264, 49040, 2059, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0845030055326574, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.031632933765649796}, {"id": 50, "seek": 15600, "start": 182.0, "end": 185.0, "text": " I'm just going to ask the agent to do it for me.", "tokens": [51664, 286, 478, 445, 516, 281, 1029, 264, 9461, 281, 360, 309, 337, 385, 13, 51814], "temperature": 0.0, "avg_logprob": -0.0845030055326574, "compression_ratio": 1.5950704225352113, "no_speech_prob": 0.031632933765649796}, {"id": 51, "seek": 18500, "start": 185.0, "end": 189.0, "text": " So I've decided to use Google's ADK repo for this example.", "tokens": [50364, 407, 286, 600, 3047, 281, 764, 3329, 311, 9135, 42, 49040, 337, 341, 1365, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09098031581976475, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.0002841700043063611}, {"id": 52, "seek": 18500, "start": 189.0, "end": 192.0, "text": " So ADK stands for agent development kit.", "tokens": [50564, 407, 9135, 42, 7382, 337, 9461, 3250, 8260, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09098031581976475, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.0002841700043063611}, {"id": 53, "seek": 18500, "start": 192.0, "end": 200.0, "text": " So I'm just going to say clone the Google ADK Python repo from GitHub.", "tokens": [50714, 407, 286, 478, 445, 516, 281, 584, 26506, 264, 3329, 9135, 42, 15329, 49040, 490, 23331, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09098031581976475, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.0002841700043063611}, {"id": 54, "seek": 18500, "start": 200.0, "end": 205.0, "text": " And I'm going to see what it does.", "tokens": [51114, 400, 286, 478, 516, 281, 536, 437, 309, 775, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09098031581976475, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.0002841700043063611}, {"id": 55, "seek": 18500, "start": 205.0, "end": 211.0, "text": " Okay, so it's searching the web trying to find the right link.", "tokens": [51364, 1033, 11, 370, 309, 311, 10808, 264, 3670, 1382, 281, 915, 264, 558, 2113, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09098031581976475, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.0002841700043063611}, {"id": 56, "seek": 21100, "start": 211.0, "end": 212.0, "text": " Awesome.", "tokens": [50364, 10391, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1463312976128232, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0042339470237493515}, {"id": 57, "seek": 21100, "start": 212.0, "end": 213.0, "text": " It found the right link.", "tokens": [50414, 467, 1352, 264, 558, 2113, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1463312976128232, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0042339470237493515}, {"id": 58, "seek": 21100, "start": 213.0, "end": 216.0, "text": " It's asking me for permission to clone it.", "tokens": [50464, 467, 311, 3365, 385, 337, 11226, 281, 26506, 309, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1463312976128232, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0042339470237493515}, {"id": 59, "seek": 21100, "start": 216.0, "end": 221.0, "text": " Go ahead.", "tokens": [50614, 1037, 2286, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1463312976128232, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0042339470237493515}, {"id": 60, "seek": 21100, "start": 221.0, "end": 222.0, "text": " Cool.", "tokens": [50864, 8561, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1463312976128232, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0042339470237493515}, {"id": 61, "seek": 21100, "start": 222.0, "end": 225.0, "text": " Yeah, and this looks like it's the agent group in action.", "tokens": [50914, 865, 11, 293, 341, 1542, 411, 309, 311, 264, 9461, 1594, 294, 3069, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1463312976128232, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0042339470237493515}, {"id": 62, "seek": 21100, "start": 225.0, "end": 228.0, "text": " It's kind of reasoning that it needs a piece of information.", "tokens": [51064, 467, 311, 733, 295, 21577, 300, 309, 2203, 257, 2522, 295, 1589, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1463312976128232, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0042339470237493515}, {"id": 63, "seek": 21100, "start": 228.0, "end": 231.0, "text": " It then chooses a tool to get that information looks like.", "tokens": [51214, 467, 550, 25963, 257, 2290, 281, 483, 300, 1589, 1542, 411, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1463312976128232, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0042339470237493515}, {"id": 64, "seek": 21100, "start": 231.0, "end": 234.0, "text": " And then it uses that information to complete the original request.", "tokens": [51364, 400, 550, 309, 4960, 300, 1589, 281, 3566, 264, 3380, 5308, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1463312976128232, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0042339470237493515}, {"id": 65, "seek": 21100, "start": 234.0, "end": 238.0, "text": " And then also at the same time, it asks you if you want to proceed.", "tokens": [51514, 400, 550, 611, 412, 264, 912, 565, 11, 309, 8962, 291, 498, 291, 528, 281, 8991, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1463312976128232, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.0042339470237493515}, {"id": 66, "seek": 23800, "start": 238.0, "end": 239.0, "text": " Checking back in.", "tokens": [50364, 6881, 278, 646, 294, 13, 50414], "temperature": 0.0, "avg_logprob": -0.1896372402415556, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.03823928162455559}, {"id": 67, "seek": 23800, "start": 239.0, "end": 242.0, "text": " So yeah, this is a super helpful workflow.", "tokens": [50414, 407, 1338, 11, 341, 307, 257, 1687, 4961, 20993, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1896372402415556, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.03823928162455559}, {"id": 68, "seek": 23800, "start": 242.0, "end": 252.0, "text": " I think the way I want to tackle this is let's imagine that we're like a new contributor to this project.", "tokens": [50564, 286, 519, 264, 636, 286, 528, 281, 14896, 341, 307, 718, 311, 3811, 300, 321, 434, 411, 257, 777, 42859, 281, 341, 1716, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1896372402415556, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.03823928162455559}, {"id": 69, "seek": 23800, "start": 252.0, "end": 259.0, "text": " What do you think is the first like what's the first thing you'd want to do if you were a new contributor to a project and you wanted to figure out how to go forward.", "tokens": [51064, 708, 360, 291, 519, 307, 264, 700, 411, 437, 311, 264, 700, 551, 291, 1116, 528, 281, 360, 498, 291, 645, 257, 777, 42859, 281, 257, 1716, 293, 291, 1415, 281, 2573, 484, 577, 281, 352, 2128, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1896372402415556, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.03823928162455559}, {"id": 70, "seek": 23800, "start": 259.0, "end": 262.0, "text": " Feel like I would want to get a high level understanding of the project.", "tokens": [51414, 14113, 411, 286, 576, 528, 281, 483, 257, 1090, 1496, 3701, 295, 264, 1716, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1896372402415556, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.03823928162455559}, {"id": 71, "seek": 23800, "start": 262.0, "end": 265.0, "text": " I think I learned best maybe from like a top down perspective.", "tokens": [51564, 286, 519, 286, 3264, 1151, 1310, 490, 411, 257, 1192, 760, 4585, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1896372402415556, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.03823928162455559}, {"id": 72, "seek": 26500, "start": 265.0, "end": 269.0, "text": " Things like text stack, how code of structure, stuff like that.", "tokens": [50364, 9514, 411, 2487, 8630, 11, 577, 3089, 295, 3877, 11, 1507, 411, 300, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16591236892255765, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.02322922833263874}, {"id": 73, "seek": 26500, "start": 269.0, "end": 273.0, "text": " Yeah, yeah, totally like in the past.", "tokens": [50564, 865, 11, 1338, 11, 3879, 411, 294, 264, 1791, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16591236892255765, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.02322922833263874}, {"id": 74, "seek": 26500, "start": 273.0, "end": 276.0, "text": " And then not so distant past actually.", "tokens": [50764, 400, 550, 406, 370, 17275, 1791, 767, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16591236892255765, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.02322922833263874}, {"id": 75, "seek": 26500, "start": 276.0, "end": 279.0, "text": " You know, you might start with like reading a read me file.", "tokens": [50914, 509, 458, 11, 291, 1062, 722, 365, 411, 3760, 257, 1401, 385, 3991, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16591236892255765, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.02322922833263874}, {"id": 76, "seek": 26500, "start": 279.0, "end": 282.0, "text": " Maybe contributing file.", "tokens": [51064, 2704, 19270, 3991, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16591236892255765, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.02322922833263874}, {"id": 77, "seek": 26500, "start": 282.0, "end": 285.0, "text": " You know, start piecing things together that way.", "tokens": [51214, 509, 458, 11, 722, 1730, 2175, 721, 1214, 300, 636, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16591236892255765, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.02322922833263874}, {"id": 78, "seek": 26500, "start": 285.0, "end": 286.0, "text": " And like.", "tokens": [51364, 400, 411, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16591236892255765, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.02322922833263874}, {"id": 79, "seek": 26500, "start": 286.0, "end": 291.0, "text": " I'm not very good at like sitting down and reading a huge amount of documentation to be quite honest.", "tokens": [51414, 286, 478, 406, 588, 665, 412, 411, 3798, 760, 293, 3760, 257, 2603, 2372, 295, 14333, 281, 312, 1596, 3245, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16591236892255765, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.02322922833263874}, {"id": 80, "seek": 29100, "start": 291.0, "end": 298.0, "text": " So instead, I'm going to ask the Gemini CLI to perform a high level audit of the entire directory.", "tokens": [50364, 407, 2602, 11, 286, 478, 516, 281, 1029, 264, 22894, 3812, 12855, 40, 281, 2042, 257, 1090, 1496, 17748, 295, 264, 2302, 21120, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10529094812821368, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.005138527136296034}, {"id": 81, "seek": 29100, "start": 298.0, "end": 301.0, "text": " So let's see.", "tokens": [50714, 407, 718, 311, 536, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10529094812821368, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.005138527136296034}, {"id": 82, "seek": 29100, "start": 301.0, "end": 305.0, "text": " I'm going to ask the Gemini CLI agent for a complete project overview.", "tokens": [50864, 286, 478, 516, 281, 1029, 264, 22894, 3812, 12855, 40, 9461, 337, 257, 3566, 1716, 12492, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10529094812821368, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.005138527136296034}, {"id": 83, "seek": 29100, "start": 305.0, "end": 312.0, "text": " I want it to tell me the purpose, the text stack and analyze architecture all in one go.", "tokens": [51064, 286, 528, 309, 281, 980, 385, 264, 4334, 11, 264, 2487, 8630, 293, 12477, 9482, 439, 294, 472, 352, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10529094812821368, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.005138527136296034}, {"id": 84, "seek": 29100, "start": 312.0, "end": 319.0, "text": " Yeah, and I feel like that's much more powerful than just relying on one read me document.", "tokens": [51414, 865, 11, 293, 286, 841, 411, 300, 311, 709, 544, 4005, 813, 445, 24140, 322, 472, 1401, 385, 4166, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10529094812821368, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.005138527136296034}, {"id": 85, "seek": 31900, "start": 319.0, "end": 322.0, "text": " This way you can kind of point at the whole directory.", "tokens": [50364, 639, 636, 291, 393, 733, 295, 935, 412, 264, 1379, 21120, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16354929483853853, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0010372726246714592}, {"id": 86, "seek": 31900, "start": 322.0, "end": 325.0, "text": " And it's cool because you're asking it to like synthesize everything.", "tokens": [50514, 400, 309, 311, 1627, 570, 291, 434, 3365, 309, 281, 411, 26617, 1125, 1203, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16354929483853853, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0010372726246714592}, {"id": 87, "seek": 31900, "start": 325.0, "end": 332.0, "text": " The source, file, the docs, all of the code into a single coherent sort of overview, right?", "tokens": [50664, 440, 4009, 11, 3991, 11, 264, 45623, 11, 439, 295, 264, 3089, 666, 257, 2167, 36239, 1333, 295, 12492, 11, 558, 30, 51014], "temperature": 0.0, "avg_logprob": -0.16354929483853853, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0010372726246714592}, {"id": 88, "seek": 31900, "start": 332.0, "end": 333.0, "text": " Yeah.", "tokens": [51014, 865, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16354929483853853, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0010372726246714592}, {"id": 89, "seek": 31900, "start": 333.0, "end": 334.0, "text": " Yeah, absolutely.", "tokens": [51064, 865, 11, 3122, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16354929483853853, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0010372726246714592}, {"id": 90, "seek": 31900, "start": 334.0, "end": 343.0, "text": " Yeah, it's like it's ingesting the entire directory to build out this like complete mental model of the project, which is super helpful.", "tokens": [51114, 865, 11, 309, 311, 411, 309, 311, 3957, 8714, 264, 2302, 21120, 281, 1322, 484, 341, 411, 3566, 4973, 2316, 295, 264, 1716, 11, 597, 307, 1687, 4961, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16354929483853853, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0010372726246714592}, {"id": 91, "seek": 31900, "start": 343.0, "end": 345.0, "text": " Okay, great.", "tokens": [51564, 1033, 11, 869, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16354929483853853, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0010372726246714592}, {"id": 92, "seek": 34500, "start": 345.0, "end": 353.0, "text": " Starting to output this looks really good and helpful.", "tokens": [50364, 16217, 281, 5598, 341, 1542, 534, 665, 293, 4961, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1818327487093731, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0037689856253564358}, {"id": 93, "seek": 34500, "start": 353.0, "end": 359.0, "text": " Yeah, identified the purpose, the text stack, core dependencies, all that good stuff.", "tokens": [50764, 865, 11, 9234, 264, 4334, 11, 264, 2487, 8630, 11, 4965, 36606, 11, 439, 300, 665, 1507, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1818327487093731, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0037689856253564358}, {"id": 94, "seek": 34500, "start": 359.0, "end": 362.0, "text": " That's like helpful to kind of know what the outlet.", "tokens": [51064, 663, 311, 411, 4961, 281, 733, 295, 458, 437, 264, 20656, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1818327487093731, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0037689856253564358}, {"id": 95, "seek": 34500, "start": 362.0, "end": 364.0, "text": " Yeah, it's really verbose what it did.", "tokens": [51214, 865, 11, 309, 311, 534, 9595, 541, 437, 309, 630, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1818327487093731, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0037689856253564358}, {"id": 96, "seek": 34500, "start": 364.0, "end": 368.0, "text": " Summary, I wouldn't have been able to write it better myself, right?", "tokens": [51314, 8626, 76, 822, 11, 286, 2759, 380, 362, 668, 1075, 281, 2464, 309, 1101, 2059, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.1818327487093731, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0037689856253564358}, {"id": 97, "seek": 34500, "start": 368.0, "end": 369.0, "text": " Yeah.", "tokens": [51514, 865, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1818327487093731, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0037689856253564358}, {"id": 98, "seek": 34500, "start": 369.0, "end": 372.0, "text": " And now for the part of the show that we've both been waiting for.", "tokens": [51564, 400, 586, 337, 264, 644, 295, 264, 855, 300, 321, 600, 1293, 668, 3806, 337, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1818327487093731, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0037689856253564358}, {"id": 99, "seek": 37200, "start": 372.0, "end": 377.0, "text": " We're going to shift gears and get a deep dive into the philosophy and future of Gemini CLI.", "tokens": [50364, 492, 434, 516, 281, 5513, 20915, 293, 483, 257, 2452, 9192, 666, 264, 10675, 293, 2027, 295, 22894, 3812, 12855, 40, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 100, "seek": 37200, "start": 377.0, "end": 383.0, "text": " We'd now like to welcome to the podcast Taylor Mullin, creator of Gemini CLI.", "tokens": [50614, 492, 1116, 586, 411, 281, 2928, 281, 264, 7367, 12060, 41621, 259, 11, 14181, 295, 22894, 3812, 12855, 40, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 101, "seek": 37200, "start": 383.0, "end": 385.0, "text": " Taylor, thank you so much for joining us.", "tokens": [50914, 12060, 11, 1309, 291, 370, 709, 337, 5549, 505, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 102, "seek": 37200, "start": 385.0, "end": 386.0, "text": " Thanks for having me.", "tokens": [51014, 2561, 337, 1419, 385, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 103, "seek": 37200, "start": 386.0, "end": 387.0, "text": " This is fun to be here.", "tokens": [51064, 639, 307, 1019, 281, 312, 510, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 104, "seek": 37200, "start": 387.0, "end": 389.0, "text": " Yeah, so let's start at the very beginning.", "tokens": [51114, 865, 11, 370, 718, 311, 722, 412, 264, 588, 2863, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 105, "seek": 37200, "start": 389.0, "end": 393.0, "text": " Could you tell us the origin story of the Gemini CLI?", "tokens": [51214, 7497, 291, 980, 505, 264, 4957, 1657, 295, 264, 22894, 3812, 12855, 40, 30, 51414], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 106, "seek": 37200, "start": 393.0, "end": 397.0, "text": " Like, what was the initial spark or inspiration that led to it?", "tokens": [51414, 1743, 11, 437, 390, 264, 5883, 9908, 420, 10249, 300, 4684, 281, 309, 30, 51614], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 107, "seek": 37200, "start": 397.0, "end": 398.0, "text": " And how did things go from there?", "tokens": [51614, 400, 577, 630, 721, 352, 490, 456, 30, 51664], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 108, "seek": 37200, "start": 398.0, "end": 399.0, "text": " I'm very curious to hear.", "tokens": [51664, 286, 478, 588, 6369, 281, 1568, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 109, "seek": 37200, "start": 399.0, "end": 401.0, "text": " Oh, yeah, it's kind of crazy.", "tokens": [51714, 876, 11, 1338, 11, 309, 311, 733, 295, 3219, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12732416705081337, "compression_ratio": 1.6139240506329113, "no_speech_prob": 0.3696984648704529}, {"id": 110, "seek": 40100, "start": 401.0, "end": 405.0, "text": " It started about a year and a half ago, weirdly enough, right?", "tokens": [50364, 467, 1409, 466, 257, 1064, 293, 257, 1922, 2057, 11, 48931, 1547, 11, 558, 30, 50564], "temperature": 0.0, "avg_logprob": -0.17894963749119494, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.03207436949014664}, {"id": 111, "seek": 40100, "start": 405.0, "end": 406.0, "text": " It's so long ago.", "tokens": [50564, 467, 311, 370, 938, 2057, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17894963749119494, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.03207436949014664}, {"id": 112, "seek": 40100, "start": 406.0, "end": 414.0, "text": " But like, at the time, I was experimenting with multi agent systems and trying to get like the most out of our elements.", "tokens": [50614, 583, 411, 11, 412, 264, 565, 11, 286, 390, 29070, 365, 4825, 9461, 3652, 293, 1382, 281, 483, 411, 264, 881, 484, 295, 527, 4959, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17894963749119494, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.03207436949014664}, {"id": 113, "seek": 40100, "start": 414.0, "end": 420.0, "text": " And so at the time, like, I had the system where the multiple personas, they were talking to each other and they were doing a lot of cool things.", "tokens": [51014, 400, 370, 412, 264, 565, 11, 411, 11, 286, 632, 264, 1185, 689, 264, 3866, 12019, 11, 436, 645, 1417, 281, 1184, 661, 293, 436, 645, 884, 257, 688, 295, 1627, 721, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17894963749119494, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.03207436949014664}, {"id": 114, "seek": 40100, "start": 420.0, "end": 422.0, "text": " And it was surfaced in a lot of different ways.", "tokens": [51314, 400, 309, 390, 9684, 3839, 294, 257, 688, 295, 819, 2098, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17894963749119494, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.03207436949014664}, {"id": 115, "seek": 40100, "start": 422.0, "end": 426.0, "text": " It was surfaced in IDE, it was surfaced in a CLI.", "tokens": [51414, 467, 390, 9684, 3839, 294, 40930, 11, 309, 390, 9684, 3839, 294, 257, 12855, 40, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17894963749119494, "compression_ratio": 1.8016194331983806, "no_speech_prob": 0.03207436949014664}, {"id": 116, "seek": 42600, "start": 426.0, "end": 428.0, "text": " It was surfaced in the web.", "tokens": [50364, 467, 390, 9684, 3839, 294, 264, 3670, 13, 50464], "temperature": 0.0, "avg_logprob": -0.15062678654988607, "compression_ratio": 1.6649214659685865, "no_speech_prob": 0.008424518629908562}, {"id": 117, "seek": 42600, "start": 428.0, "end": 434.0, "text": " And as I was playing with these things, the thing that really stuck for me was the CLI.", "tokens": [50464, 400, 382, 286, 390, 2433, 365, 613, 721, 11, 264, 551, 300, 534, 5541, 337, 385, 390, 264, 12855, 40, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15062678654988607, "compression_ratio": 1.6649214659685865, "no_speech_prob": 0.008424518629908562}, {"id": 118, "seek": 42600, "start": 434.0, "end": 437.0, "text": " It was so easy to use, so lightweight.", "tokens": [50764, 467, 390, 370, 1858, 281, 764, 11, 370, 22052, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15062678654988607, "compression_ratio": 1.6649214659685865, "no_speech_prob": 0.008424518629908562}, {"id": 119, "seek": 42600, "start": 437.0, "end": 447.0, "text": " But at the time, like, it took 30 requests, like one user request resulted in like 30 LLM requests behind the scenes.", "tokens": [50914, 583, 412, 264, 565, 11, 411, 11, 309, 1890, 2217, 12475, 11, 411, 472, 4195, 5308, 18753, 294, 411, 2217, 441, 43, 44, 12475, 2261, 264, 8026, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15062678654988607, "compression_ratio": 1.6649214659685865, "no_speech_prob": 0.008424518629908562}, {"id": 120, "seek": 42600, "start": 447.0, "end": 450.0, "text": " And those 30 requests was a lot for the time.", "tokens": [51414, 400, 729, 2217, 12475, 390, 257, 688, 337, 264, 565, 13, 51564], "temperature": 0.0, "avg_logprob": -0.15062678654988607, "compression_ratio": 1.6649214659685865, "no_speech_prob": 0.008424518629908562}, {"id": 121, "seek": 45000, "start": 450.0, "end": 453.0, "text": " It took a minute and a half to respond to anything.", "tokens": [50364, 467, 1890, 257, 3456, 293, 257, 1922, 281, 4196, 281, 1340, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13018502638890192, "compression_ratio": 1.65625, "no_speech_prob": 0.2822306752204895}, {"id": 122, "seek": 45000, "start": 453.0, "end": 455.0, "text": " And the quality was there.", "tokens": [50514, 400, 264, 3125, 390, 456, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13018502638890192, "compression_ratio": 1.65625, "no_speech_prob": 0.2822306752204895}, {"id": 123, "seek": 45000, "start": 455.0, "end": 459.0, "text": " Like, this was something at the time that was just too much.", "tokens": [50614, 1743, 11, 341, 390, 746, 412, 264, 565, 300, 390, 445, 886, 709, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13018502638890192, "compression_ratio": 1.65625, "no_speech_prob": 0.2822306752204895}, {"id": 124, "seek": 45000, "start": 459.0, "end": 462.0, "text": " Like, we also have limited context window.", "tokens": [50814, 1743, 11, 321, 611, 362, 5567, 4319, 4910, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13018502638890192, "compression_ratio": 1.65625, "no_speech_prob": 0.2822306752204895}, {"id": 125, "seek": 45000, "start": 462.0, "end": 470.0, "text": " And so in that realm, it being a CLI and it having all these constraints, we scrapped the project.", "tokens": [50964, 400, 370, 294, 300, 15355, 11, 309, 885, 257, 12855, 40, 293, 309, 1419, 439, 613, 18491, 11, 321, 13943, 3320, 264, 1716, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13018502638890192, "compression_ratio": 1.65625, "no_speech_prob": 0.2822306752204895}, {"id": 126, "seek": 45000, "start": 470.0, "end": 472.0, "text": " And that was kind of challenging at the time.", "tokens": [51364, 400, 300, 390, 733, 295, 7595, 412, 264, 565, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13018502638890192, "compression_ratio": 1.65625, "no_speech_prob": 0.2822306752204895}, {"id": 127, "seek": 45000, "start": 472.0, "end": 474.0, "text": " It was almost a little bit too early.", "tokens": [51464, 467, 390, 1920, 257, 707, 857, 886, 2440, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13018502638890192, "compression_ratio": 1.65625, "no_speech_prob": 0.2822306752204895}, {"id": 128, "seek": 45000, "start": 474.0, "end": 475.0, "text": " Yeah.", "tokens": [51564, 865, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13018502638890192, "compression_ratio": 1.65625, "no_speech_prob": 0.2822306752204895}, {"id": 129, "seek": 47500, "start": 475.0, "end": 480.0, "text": " It was one of those things like, okay, no one's going to want to use this because it cost too much.", "tokens": [50364, 467, 390, 472, 295, 729, 721, 411, 11, 1392, 11, 572, 472, 311, 516, 281, 528, 281, 764, 341, 570, 309, 2063, 886, 709, 13, 50614], "temperature": 0.0, "avg_logprob": -0.18017669222248134, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.09134896844625473}, {"id": 130, "seek": 47500, "start": 480.0, "end": 482.0, "text": " It couldn't absorb enough information.", "tokens": [50614, 467, 2809, 380, 15631, 1547, 1589, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18017669222248134, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.09134896844625473}, {"id": 131, "seek": 47500, "start": 482.0, "end": 488.0, "text": " And it took too long to respond, which is kind of a trippy thing like fast forwarding today, right?", "tokens": [50714, 400, 309, 1890, 886, 938, 281, 4196, 11, 597, 307, 733, 295, 257, 1376, 7966, 551, 411, 2370, 2128, 278, 965, 11, 558, 30, 51014], "temperature": 0.0, "avg_logprob": -0.18017669222248134, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.09134896844625473}, {"id": 132, "seek": 47500, "start": 488.0, "end": 491.0, "text": " We're very much used to, you have deep research, like you click a button.", "tokens": [51014, 492, 434, 588, 709, 1143, 281, 11, 291, 362, 2452, 2132, 11, 411, 291, 2052, 257, 2960, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18017669222248134, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.09134896844625473}, {"id": 133, "seek": 47500, "start": 491.0, "end": 495.0, "text": " And then you go off and you get a coffee and you come back and hopefully it's done.", "tokens": [51164, 400, 550, 291, 352, 766, 293, 291, 483, 257, 4982, 293, 291, 808, 646, 293, 4696, 309, 311, 1096, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18017669222248134, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.09134896844625473}, {"id": 134, "seek": 47500, "start": 495.0, "end": 496.0, "text": " And it's crazy.", "tokens": [51364, 400, 309, 311, 3219, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18017669222248134, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.09134896844625473}, {"id": 135, "seek": 47500, "start": 496.0, "end": 498.0, "text": " It's so different.", "tokens": [51414, 467, 311, 370, 819, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18017669222248134, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.09134896844625473}, {"id": 136, "seek": 47500, "start": 498.0, "end": 502.0, "text": " But during that time, like, it was too early.", "tokens": [51514, 583, 1830, 300, 565, 11, 411, 11, 309, 390, 886, 2440, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18017669222248134, "compression_ratio": 1.6620209059233448, "no_speech_prob": 0.09134896844625473}, {"id": 137, "seek": 50200, "start": 502.0, "end": 508.0, "text": " And then today, like me, Google and me, I'm really trying to like build this future of developer tools.", "tokens": [50364, 400, 550, 965, 11, 411, 385, 11, 3329, 293, 385, 11, 286, 478, 534, 1382, 281, 411, 1322, 341, 2027, 295, 10754, 3873, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1389085619073165, "compression_ratio": 1.684, "no_speech_prob": 0.0020470910239964724}, {"id": 138, "seek": 50200, "start": 508.0, "end": 514.0, "text": " Like, there was this ecosystem of CLIs that was just taking hold in the community, right?", "tokens": [50664, 1743, 11, 456, 390, 341, 11311, 295, 12855, 6802, 300, 390, 445, 1940, 1797, 294, 264, 1768, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.1389085619073165, "compression_ratio": 1.684, "no_speech_prob": 0.0020470910239964724}, {"id": 139, "seek": 50200, "start": 514.0, "end": 518.0, "text": " And it really proved that people are willing to wait.", "tokens": [50964, 400, 309, 534, 14617, 300, 561, 366, 4950, 281, 1699, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1389085619073165, "compression_ratio": 1.684, "no_speech_prob": 0.0020470910239964724}, {"id": 140, "seek": 50200, "start": 518.0, "end": 522.0, "text": " People are willing to use a CLI.", "tokens": [51164, 3432, 366, 4950, 281, 764, 257, 12855, 40, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1389085619073165, "compression_ratio": 1.684, "no_speech_prob": 0.0020470910239964724}, {"id": 141, "seek": 50200, "start": 522.0, "end": 525.0, "text": " And they find it, they found it as compelling as they I did back then.", "tokens": [51364, 400, 436, 915, 309, 11, 436, 1352, 309, 382, 20050, 382, 436, 286, 630, 646, 550, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1389085619073165, "compression_ratio": 1.684, "no_speech_prob": 0.0020470910239964724}, {"id": 142, "seek": 50200, "start": 525.0, "end": 527.0, "text": " It's like, when all this started to happen, I'm like, oh my goodness.", "tokens": [51514, 467, 311, 411, 11, 562, 439, 341, 1409, 281, 1051, 11, 286, 478, 411, 11, 1954, 452, 8387, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1389085619073165, "compression_ratio": 1.684, "no_speech_prob": 0.0020470910239964724}, {"id": 143, "seek": 52700, "start": 527.0, "end": 532.0, "text": " Maybe I was just too early and maybe we should try and give this another go.", "tokens": [50364, 2704, 286, 390, 445, 886, 2440, 293, 1310, 321, 820, 853, 293, 976, 341, 1071, 352, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10726978665306455, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.46896058320999146}, {"id": 144, "seek": 52700, "start": 532.0, "end": 538.0, "text": " And so that like, I really grounded at least the motivation of saying, okay, I've done this before.", "tokens": [50614, 400, 370, 300, 411, 11, 286, 534, 23535, 412, 1935, 264, 12335, 295, 1566, 11, 1392, 11, 286, 600, 1096, 341, 949, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10726978665306455, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.46896058320999146}, {"id": 145, "seek": 52700, "start": 538.0, "end": 545.0, "text": " And like, I've learned a lot over my entire career of what it means to build developer tools and what it means.", "tokens": [50914, 400, 411, 11, 286, 600, 3264, 257, 688, 670, 452, 2302, 3988, 295, 437, 309, 1355, 281, 1322, 10754, 3873, 293, 437, 309, 1355, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10726978665306455, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.46896058320999146}, {"id": 146, "seek": 52700, "start": 545.0, "end": 550.0, "text": " It's like, build something that can kind of span a lot of ecosystems.", "tokens": [51264, 467, 311, 411, 11, 1322, 746, 300, 393, 733, 295, 16174, 257, 688, 295, 32647, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10726978665306455, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.46896058320999146}, {"id": 147, "seek": 52700, "start": 550.0, "end": 553.0, "text": " And so we started Gemini CLI from that point.", "tokens": [51514, 400, 370, 321, 1409, 22894, 3812, 12855, 40, 490, 300, 935, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10726978665306455, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.46896058320999146}, {"id": 148, "seek": 55300, "start": 553.0, "end": 557.0, "text": " So it's like, I basically, I went dark, obviously, I went dark for like a week.", "tokens": [50364, 407, 309, 311, 411, 11, 286, 1936, 11, 286, 1437, 2877, 11, 2745, 11, 286, 1437, 2877, 337, 411, 257, 1243, 13, 50564], "temperature": 0.0, "avg_logprob": -0.20075535936420466, "compression_ratio": 1.8350877192982455, "no_speech_prob": 0.1329757422208786}, {"id": 149, "seek": 55300, "start": 557.0, "end": 560.0, "text": " And I'm like, okay, I'm going to do this.", "tokens": [50564, 400, 286, 478, 411, 11, 1392, 11, 286, 478, 516, 281, 360, 341, 13, 50714], "temperature": 0.0, "avg_logprob": -0.20075535936420466, "compression_ratio": 1.8350877192982455, "no_speech_prob": 0.1329757422208786}, {"id": 150, "seek": 55300, "start": 560.0, "end": 564.0, "text": " I have this idea that went dark for a week, talked to no one, spent every waking second,", "tokens": [50714, 286, 362, 341, 1558, 300, 1437, 2877, 337, 257, 1243, 11, 2825, 281, 572, 472, 11, 4418, 633, 20447, 1150, 11, 50914], "temperature": 0.0, "avg_logprob": -0.20075535936420466, "compression_ratio": 1.8350877192982455, "no_speech_prob": 0.1329757422208786}, {"id": 151, "seek": 55300, "start": 564.0, "end": 567.0, "text": " was getting like five to six hours of sleep in the night.", "tokens": [50914, 390, 1242, 411, 1732, 281, 2309, 2496, 295, 2817, 294, 264, 1818, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20075535936420466, "compression_ratio": 1.8350877192982455, "no_speech_prob": 0.1329757422208786}, {"id": 152, "seek": 55300, "start": 567.0, "end": 570.0, "text": " And this was like, I think it was like Saturday through Saturday.", "tokens": [51064, 400, 341, 390, 411, 11, 286, 519, 309, 390, 411, 8803, 807, 8803, 13, 51214], "temperature": 0.0, "avg_logprob": -0.20075535936420466, "compression_ratio": 1.8350877192982455, "no_speech_prob": 0.1329757422208786}, {"id": 153, "seek": 55300, "start": 570.0, "end": 572.0, "text": " Saturday through Sunday or something like that.", "tokens": [51214, 8803, 807, 7776, 420, 746, 411, 300, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20075535936420466, "compression_ratio": 1.8350877192982455, "no_speech_prob": 0.1329757422208786}, {"id": 154, "seek": 55300, "start": 572.0, "end": 576.0, "text": " And then like that Monday, I had a prototype, did a video.", "tokens": [51314, 400, 550, 411, 300, 8138, 11, 286, 632, 257, 19475, 11, 630, 257, 960, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20075535936420466, "compression_ratio": 1.8350877192982455, "no_speech_prob": 0.1329757422208786}, {"id": 155, "seek": 55300, "start": 576.0, "end": 581.0, "text": " I put it out and Googlers started just going like wild for it.", "tokens": [51514, 286, 829, 309, 484, 293, 45005, 11977, 1409, 445, 516, 411, 4868, 337, 309, 13, 51764], "temperature": 0.0, "avg_logprob": -0.20075535936420466, "compression_ratio": 1.8350877192982455, "no_speech_prob": 0.1329757422208786}, {"id": 156, "seek": 55300, "start": 581.0, "end": 582.0, "text": " It was super cool.", "tokens": [51764, 467, 390, 1687, 1627, 13, 51814], "temperature": 0.0, "avg_logprob": -0.20075535936420466, "compression_ratio": 1.8350877192982455, "no_speech_prob": 0.1329757422208786}, {"id": 157, "seek": 58200, "start": 582.0, "end": 584.0, "text": " Yeah, thanks so much for sharing that.", "tokens": [50364, 865, 11, 3231, 370, 709, 337, 5414, 300, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08566847634971689, "compression_ratio": 1.5818815331010454, "no_speech_prob": 0.0011334752198308706}, {"id": 158, "seek": 58200, "start": 584.0, "end": 590.0, "text": " And so I guess building on that, you know, making the Gemini CLI open source was a very deliberate choice, I think,", "tokens": [50464, 400, 370, 286, 2041, 2390, 322, 300, 11, 291, 458, 11, 1455, 264, 22894, 3812, 12855, 40, 1269, 4009, 390, 257, 588, 30515, 3922, 11, 286, 519, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08566847634971689, "compression_ratio": 1.5818815331010454, "no_speech_prob": 0.0011334752198308706}, {"id": 159, "seek": 58200, "start": 590.0, "end": 595.0, "text": " which you've linked to security and learning and a whole host of other things.", "tokens": [50764, 597, 291, 600, 9408, 281, 3825, 293, 2539, 293, 257, 1379, 3975, 295, 661, 721, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08566847634971689, "compression_ratio": 1.5818815331010454, "no_speech_prob": 0.0011334752198308706}, {"id": 160, "seek": 58200, "start": 595.0, "end": 604.0, "text": " So how does this open, trust-based approach contrast with the more like black box nature of other maybe major AI tools out there?", "tokens": [51014, 407, 577, 775, 341, 1269, 11, 3361, 12, 6032, 3109, 8712, 365, 264, 544, 411, 2211, 2424, 3687, 295, 661, 1310, 2563, 7318, 3873, 484, 456, 30, 51464], "temperature": 0.0, "avg_logprob": -0.08566847634971689, "compression_ratio": 1.5818815331010454, "no_speech_prob": 0.0011334752198308706}, {"id": 161, "seek": 58200, "start": 604.0, "end": 610.0, "text": " And what kind of ecosystem do you hope that this will foster the transparency will foster?", "tokens": [51464, 400, 437, 733, 295, 11311, 360, 291, 1454, 300, 341, 486, 17114, 264, 17131, 486, 17114, 30, 51764], "temperature": 0.0, "avg_logprob": -0.08566847634971689, "compression_ratio": 1.5818815331010454, "no_speech_prob": 0.0011334752198308706}, {"id": 162, "seek": 61000, "start": 610.0, "end": 612.0, "text": " Yeah, it's like to be frigate.", "tokens": [50364, 865, 11, 309, 311, 411, 281, 312, 34697, 473, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1334449713178676, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.010923611000180244}, {"id": 163, "seek": 61000, "start": 612.0, "end": 615.0, "text": " I started my career in open source.", "tokens": [50464, 286, 1409, 452, 3988, 294, 1269, 4009, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1334449713178676, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.010923611000180244}, {"id": 164, "seek": 61000, "start": 615.0, "end": 621.0, "text": " Like I have always been in developer tooling or framework to one thing and the other and open source has been like front and center.", "tokens": [50614, 1743, 286, 362, 1009, 668, 294, 10754, 46593, 420, 8388, 281, 472, 551, 293, 264, 661, 293, 1269, 4009, 575, 668, 411, 1868, 293, 3056, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1334449713178676, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.010923611000180244}, {"id": 165, "seek": 61000, "start": 621.0, "end": 625.0, "text": " So like from day zero, that was a key thing in my head.", "tokens": [50914, 407, 411, 490, 786, 4018, 11, 300, 390, 257, 2141, 551, 294, 452, 1378, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1334449713178676, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.010923611000180244}, {"id": 166, "seek": 61000, "start": 625.0, "end": 628.0, "text": " But most folks feel like, oh, it's open source.", "tokens": [51114, 583, 881, 4024, 841, 411, 11, 1954, 11, 309, 311, 1269, 4009, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1334449713178676, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.010923611000180244}, {"id": 167, "seek": 61000, "start": 628.0, "end": 630.0, "text": " Why would you not go ahead and do it?", "tokens": [51264, 1545, 576, 291, 406, 352, 2286, 293, 360, 309, 30, 51364], "temperature": 0.0, "avg_logprob": -0.1334449713178676, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.010923611000180244}, {"id": 168, "seek": 61000, "start": 630.0, "end": 632.0, "text": " Open source is not free.", "tokens": [51364, 7238, 4009, 307, 406, 1737, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1334449713178676, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.010923611000180244}, {"id": 169, "seek": 61000, "start": 632.0, "end": 633.0, "text": " It's not free.", "tokens": [51464, 467, 311, 406, 1737, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1334449713178676, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.010923611000180244}, {"id": 170, "seek": 61000, "start": 633.0, "end": 636.0, "text": " It's actually a very challenging thing to get right.", "tokens": [51514, 467, 311, 767, 257, 588, 7595, 551, 281, 483, 558, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1334449713178676, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.010923611000180244}, {"id": 171, "seek": 61000, "start": 636.0, "end": 639.0, "text": " But oh my gosh, it's so rewarding when you do.", "tokens": [51664, 583, 1954, 452, 6502, 11, 309, 311, 370, 20063, 562, 291, 360, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1334449713178676, "compression_ratio": 1.7240143369175627, "no_speech_prob": 0.010923611000180244}, {"id": 172, "seek": 63900, "start": 639.0, "end": 645.0, "text": " So like knowing this, it was always this question like, okay, let's weigh the actual balance.", "tokens": [50364, 407, 411, 5276, 341, 11, 309, 390, 1009, 341, 1168, 411, 11, 1392, 11, 718, 311, 13843, 264, 3539, 4772, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16337516091086648, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.0019994480535387993}, {"id": 173, "seek": 63900, "start": 645.0, "end": 652.0, "text": " Like should it be open source should not be in the answers ended being like obviously it should be open source, like especially with a CLI tool like this.", "tokens": [50664, 1743, 820, 309, 312, 1269, 4009, 820, 406, 312, 294, 264, 6338, 4590, 885, 411, 2745, 309, 820, 312, 1269, 4009, 11, 411, 2318, 365, 257, 12855, 40, 2290, 411, 341, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16337516091086648, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.0019994480535387993}, {"id": 174, "seek": 63900, "start": 652.0, "end": 657.0, "text": " That's forward like for a wide variety of people and of course developers as well.", "tokens": [51014, 663, 311, 2128, 411, 337, 257, 4874, 5673, 295, 561, 293, 295, 1164, 8849, 382, 731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16337516091086648, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.0019994480535387993}, {"id": 175, "seek": 63900, "start": 657.0, "end": 663.0, "text": " But something that people could see and could understand, okay, it's running on my box.", "tokens": [51264, 583, 746, 300, 561, 727, 536, 293, 727, 1223, 11, 1392, 11, 309, 311, 2614, 322, 452, 2424, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16337516091086648, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.0019994480535387993}, {"id": 176, "seek": 63900, "start": 663.0, "end": 665.0, "text": " It has a lot of capability.", "tokens": [51564, 467, 575, 257, 688, 295, 13759, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16337516091086648, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.0019994480535387993}, {"id": 177, "seek": 63900, "start": 665.0, "end": 667.0, "text": " So what does it do and can I trust it?", "tokens": [51664, 407, 437, 775, 309, 360, 293, 393, 286, 3361, 309, 30, 51764], "temperature": 0.0, "avg_logprob": -0.16337516091086648, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.0019994480535387993}, {"id": 178, "seek": 66700, "start": 667.0, "end": 670.0, "text": " It's like that kind of like you mentioned the security aspect.", "tokens": [50364, 467, 311, 411, 300, 733, 295, 411, 291, 2835, 264, 3825, 4171, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10735938745901125, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.030954765155911446}, {"id": 179, "seek": 66700, "start": 670.0, "end": 674.0, "text": " That was one of our biggest reasons for like open sourcing this.", "tokens": [50514, 663, 390, 472, 295, 527, 3880, 4112, 337, 411, 1269, 11006, 2175, 341, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10735938745901125, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.030954765155911446}, {"id": 180, "seek": 66700, "start": 674.0, "end": 677.0, "text": " We want people to see exactly how it operates.", "tokens": [50714, 492, 528, 561, 281, 536, 2293, 577, 309, 22577, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10735938745901125, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.030954765155911446}, {"id": 181, "seek": 66700, "start": 677.0, "end": 679.0, "text": " So they can have they can have trust.", "tokens": [50864, 407, 436, 393, 362, 436, 393, 362, 3361, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10735938745901125, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.030954765155911446}, {"id": 182, "seek": 66700, "start": 679.0, "end": 681.0, "text": " They know we're not doing anything behind the scenes.", "tokens": [50964, 814, 458, 321, 434, 406, 884, 1340, 2261, 264, 8026, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10735938745901125, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.030954765155911446}, {"id": 183, "seek": 66700, "start": 681.0, "end": 683.0, "text": " They know exactly what's happening.", "tokens": [51064, 814, 458, 2293, 437, 311, 2737, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10735938745901125, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.030954765155911446}, {"id": 184, "seek": 66700, "start": 683.0, "end": 687.0, "text": " And also means that if we make a mistake.", "tokens": [51164, 400, 611, 1355, 300, 498, 321, 652, 257, 6146, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10735938745901125, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.030954765155911446}, {"id": 185, "seek": 66700, "start": 687.0, "end": 692.0, "text": " That we can fix it that we have the entire community to help keep us grounded.", "tokens": [51364, 663, 321, 393, 3191, 309, 300, 321, 362, 264, 2302, 1768, 281, 854, 1066, 505, 23535, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10735938745901125, "compression_ratio": 1.6852589641434264, "no_speech_prob": 0.030954765155911446}, {"id": 186, "seek": 69200, "start": 692.0, "end": 697.0, "text": " And what makes the most secure sense we to be frank, we struggle to keep up.", "tokens": [50364, 400, 437, 1669, 264, 881, 7144, 2020, 321, 281, 312, 10455, 11, 321, 7799, 281, 1066, 493, 13, 50614], "temperature": 0.0, "avg_logprob": -0.15684584609600677, "compression_ratio": 1.7520325203252032, "no_speech_prob": 0.25725698471069336}, {"id": 187, "seek": 69200, "start": 697.0, "end": 701.0, "text": " We totally struggle to keep up with all the energy they give.", "tokens": [50614, 492, 3879, 7799, 281, 1066, 493, 365, 439, 264, 2281, 436, 976, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15684584609600677, "compression_ratio": 1.7520325203252032, "no_speech_prob": 0.25725698471069336}, {"id": 188, "seek": 69200, "start": 701.0, "end": 705.0, "text": " But to be frank, it's one of the most important things that we have.", "tokens": [50814, 583, 281, 312, 10455, 11, 309, 311, 472, 295, 264, 881, 1021, 721, 300, 321, 362, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15684584609600677, "compression_ratio": 1.7520325203252032, "no_speech_prob": 0.25725698471069336}, {"id": 189, "seek": 69200, "start": 705.0, "end": 713.0, "text": " And so when people ask me like, what is the number one thing that's on your mind for Jim and I see a line like it's our open source community.", "tokens": [51014, 400, 370, 562, 561, 1029, 385, 411, 11, 437, 307, 264, 1230, 472, 551, 300, 311, 322, 428, 1575, 337, 6637, 293, 286, 536, 257, 1622, 411, 309, 311, 527, 1269, 4009, 1768, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15684584609600677, "compression_ratio": 1.7520325203252032, "no_speech_prob": 0.25725698471069336}, {"id": 190, "seek": 69200, "start": 713.0, "end": 715.0, "text": " Like by far, it's number one.", "tokens": [51414, 1743, 538, 1400, 11, 309, 311, 1230, 472, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15684584609600677, "compression_ratio": 1.7520325203252032, "no_speech_prob": 0.25725698471069336}, {"id": 191, "seek": 69200, "start": 715.0, "end": 718.0, "text": " And that's like, I think we've shut down the team.", "tokens": [51514, 400, 300, 311, 411, 11, 286, 519, 321, 600, 5309, 760, 264, 1469, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15684584609600677, "compression_ratio": 1.7520325203252032, "no_speech_prob": 0.25725698471069336}, {"id": 192, "seek": 71800, "start": 718.0, "end": 726.0, "text": " Like literally everyone on the team for like days in order to make sure that we can try and keep up with the amount of traction and the amount of energy coming at us.", "tokens": [50364, 1743, 3736, 1518, 322, 264, 1469, 337, 411, 1708, 294, 1668, 281, 652, 988, 300, 321, 393, 853, 293, 1066, 493, 365, 264, 2372, 295, 23558, 293, 264, 2372, 295, 2281, 1348, 412, 505, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09964688886113528, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005585659295320511}, {"id": 193, "seek": 71800, "start": 726.0, "end": 728.0, "text": " But I think that's because we see the value.", "tokens": [50764, 583, 286, 519, 300, 311, 570, 321, 536, 264, 2158, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09964688886113528, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005585659295320511}, {"id": 194, "seek": 71800, "start": 728.0, "end": 733.0, "text": " We see how valuable it is to kind of build this in the open together.", "tokens": [50864, 492, 536, 577, 8263, 309, 307, 281, 733, 295, 1322, 341, 294, 264, 1269, 1214, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09964688886113528, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005585659295320511}, {"id": 195, "seek": 71800, "start": 733.0, "end": 737.0, "text": " So that folks have that confidence that we're doing the right thing.", "tokens": [51114, 407, 300, 4024, 362, 300, 6687, 300, 321, 434, 884, 264, 558, 551, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09964688886113528, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005585659295320511}, {"id": 196, "seek": 71800, "start": 737.0, "end": 741.0, "text": " And we have that confidence we're building the right things as well.", "tokens": [51314, 400, 321, 362, 300, 6687, 321, 434, 2390, 264, 558, 721, 382, 731, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09964688886113528, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005585659295320511}, {"id": 197, "seek": 71800, "start": 741.0, "end": 742.0, "text": " And it's funny.", "tokens": [51514, 400, 309, 311, 4074, 13, 51564], "temperature": 0.0, "avg_logprob": -0.09964688886113528, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005585659295320511}, {"id": 198, "seek": 71800, "start": 742.0, "end": 744.0, "text": " We actually still do the updates today.", "tokens": [51564, 492, 767, 920, 360, 264, 9205, 965, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09964688886113528, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.005585659295320511}, {"id": 199, "seek": 74400, "start": 744.0, "end": 755.0, "text": " Actually on all of our socials will post every single week on Wednesdays will post like 100 to 150 features weekly features bugs enhancements all the above.", "tokens": [50364, 5135, 322, 439, 295, 527, 2093, 82, 486, 2183, 633, 2167, 1243, 322, 10579, 82, 486, 2183, 411, 2319, 281, 8451, 4122, 12460, 4122, 15120, 11985, 1117, 439, 264, 3673, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1650106112162272, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.053623467683792114}, {"id": 200, "seek": 74400, "start": 755.0, "end": 758.0, "text": " You said 150 features a week, right?", "tokens": [50914, 509, 848, 8451, 4122, 257, 1243, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.1650106112162272, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.053623467683792114}, {"id": 201, "seek": 74400, "start": 758.0, "end": 761.0, "text": " That's a lot.", "tokens": [51064, 663, 311, 257, 688, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1650106112162272, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.053623467683792114}, {"id": 202, "seek": 74400, "start": 761.0, "end": 766.0, "text": " What are the mechanisms that you use you and the team used to make that possible?", "tokens": [51214, 708, 366, 264, 15902, 300, 291, 764, 291, 293, 264, 1469, 1143, 281, 652, 300, 1944, 30, 51464], "temperature": 0.0, "avg_logprob": -0.1650106112162272, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.053623467683792114}, {"id": 203, "seek": 76600, "start": 766.0, "end": 770.0, "text": " Because we use it to build itself like it allows us to.", "tokens": [50364, 1436, 321, 764, 309, 281, 1322, 2564, 411, 309, 4045, 505, 281, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11563400052628427, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.08982916176319122}, {"id": 204, "seek": 76600, "start": 770.0, "end": 778.0, "text": " Allow us to do a lot more like one of the coolest things I think that we started doing is when you teach it to boot itself.", "tokens": [50564, 32225, 505, 281, 360, 257, 688, 544, 411, 472, 295, 264, 22013, 721, 286, 519, 300, 321, 1409, 884, 307, 562, 291, 2924, 309, 281, 11450, 2564, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11563400052628427, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.08982916176319122}, {"id": 205, "seek": 76600, "start": 778.0, "end": 783.0, "text": " It means it can spin up parallel like threads simultaneously.", "tokens": [50964, 467, 1355, 309, 393, 6060, 493, 8952, 411, 19314, 16561, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11563400052628427, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.08982916176319122}, {"id": 206, "seek": 76600, "start": 783.0, "end": 786.0, "text": " And each of those can go ahead and tackle different problems.", "tokens": [51214, 400, 1184, 295, 729, 393, 352, 2286, 293, 14896, 819, 2740, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11563400052628427, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.08982916176319122}, {"id": 207, "seek": 76600, "start": 786.0, "end": 794.0, "text": " Combine that with work get work trees and you're going even further so far gone are the days of being able to ship like twice a year.", "tokens": [51364, 25939, 533, 300, 365, 589, 483, 589, 5852, 293, 291, 434, 516, 754, 3052, 370, 1400, 2780, 366, 264, 1708, 295, 885, 1075, 281, 5374, 411, 6091, 257, 1064, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11563400052628427, "compression_ratio": 1.6807692307692308, "no_speech_prob": 0.08982916176319122}, {"id": 208, "seek": 79400, "start": 794.0, "end": 797.0, "text": " Which is a huge part of software historically.", "tokens": [50364, 3013, 307, 257, 2603, 644, 295, 4722, 16180, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1132833712568907, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.000673117465339601}, {"id": 209, "seek": 79400, "start": 797.0, "end": 802.0, "text": " Because in AI every single week is like an insane amount of time.", "tokens": [50514, 1436, 294, 7318, 633, 2167, 1243, 307, 411, 364, 10838, 2372, 295, 565, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1132833712568907, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.000673117465339601}, {"id": 210, "seek": 79400, "start": 802.0, "end": 805.0, "text": " Every single month is like yeah.", "tokens": [50764, 2048, 2167, 1618, 307, 411, 1338, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1132833712568907, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.000673117465339601}, {"id": 211, "seek": 79400, "start": 805.0, "end": 807.0, "text": " Absolutely.", "tokens": [50914, 7021, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1132833712568907, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.000673117465339601}, {"id": 212, "seek": 79400, "start": 807.0, "end": 812.0, "text": " So I think it's it's honestly it's just that it's a mindset such cultural thing that we've built.", "tokens": [51014, 407, 286, 519, 309, 311, 309, 311, 6095, 309, 311, 445, 300, 309, 311, 257, 12543, 1270, 6988, 551, 300, 321, 600, 3094, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1132833712568907, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.000673117465339601}, {"id": 213, "seek": 79400, "start": 812.0, "end": 816.0, "text": " And I also want to kind of acknowledge our open source community again as well.", "tokens": [51264, 400, 286, 611, 528, 281, 733, 295, 10692, 527, 1269, 4009, 1768, 797, 382, 731, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1132833712568907, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.000673117465339601}, {"id": 214, "seek": 79400, "start": 816.0, "end": 823.0, "text": " Which is they really help make it possible because without folks really contributing helping each other out even.", "tokens": [51464, 3013, 307, 436, 534, 854, 652, 309, 1944, 570, 1553, 4024, 534, 19270, 4315, 1184, 661, 484, 754, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1132833712568907, "compression_ratio": 1.6209386281588447, "no_speech_prob": 0.000673117465339601}, {"id": 215, "seek": 82300, "start": 823.0, "end": 833.0, "text": " I don't know how easy it would it would be like we put human eyes on every single one of the changes that go and we don't we're not just like letting things go without looking at them.", "tokens": [50364, 286, 500, 380, 458, 577, 1858, 309, 576, 309, 576, 312, 411, 321, 829, 1952, 2575, 322, 633, 2167, 472, 295, 264, 2962, 300, 352, 293, 321, 500, 380, 321, 434, 406, 445, 411, 8295, 721, 352, 1553, 1237, 412, 552, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08022736259128736, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.0185751523822546}, {"id": 216, "seek": 82300, "start": 833.0, "end": 843.0, "text": " It's just we have a lot of really passionate really smart capable and motivated people to kind of make the right decisions at scale.", "tokens": [50864, 467, 311, 445, 321, 362, 257, 688, 295, 534, 11410, 534, 4069, 8189, 293, 14515, 561, 281, 733, 295, 652, 264, 558, 5327, 412, 4373, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08022736259128736, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.0185751523822546}, {"id": 217, "seek": 82300, "start": 843.0, "end": 852.0, "text": " You use Gemini CLI to build Gemini CLI so I'm curious to tell a little bit more about how that journey started and and kind of what was your favorite part of it.", "tokens": [51364, 509, 764, 22894, 3812, 12855, 40, 281, 1322, 22894, 3812, 12855, 40, 370, 286, 478, 6369, 281, 980, 257, 707, 857, 544, 466, 577, 300, 4671, 1409, 293, 293, 733, 295, 437, 390, 428, 2954, 644, 295, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08022736259128736, "compression_ratio": 1.657439446366782, "no_speech_prob": 0.0185751523822546}, {"id": 218, "seek": 85200, "start": 852.0, "end": 858.0, "text": " Yeah, it's kind of nuts like I still remember actually the first feature it built for itself.", "tokens": [50364, 865, 11, 309, 311, 733, 295, 10483, 411, 286, 920, 1604, 767, 264, 700, 4111, 309, 3094, 337, 2564, 13, 50664], "temperature": 0.0, "avg_logprob": -0.22295904631661898, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.003759225830435753}, {"id": 219, "seek": 85200, "start": 858.0, "end": 865.0, "text": " Which is it's kind of crazy it was I see it's so long but it really wasn't that long ago.", "tokens": [50664, 3013, 307, 309, 311, 733, 295, 3219, 309, 390, 286, 536, 309, 311, 370, 938, 457, 309, 534, 2067, 380, 300, 938, 2057, 13, 51014], "temperature": 0.0, "avg_logprob": -0.22295904631661898, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.003759225830435753}, {"id": 220, "seek": 85200, "start": 865.0, "end": 871.0, "text": " But early on we knew that markdown rendering was going to be important right.", "tokens": [51014, 583, 2440, 322, 321, 2586, 300, 1491, 5093, 22407, 390, 516, 281, 312, 1021, 558, 13, 51314], "temperature": 0.0, "avg_logprob": -0.22295904631661898, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.003759225830435753}, {"id": 221, "seek": 85200, "start": 871.0, "end": 878.0, "text": " So I'm sitting there and I'm trying to build out its ability to render mark down a set of seeing the raw markdowns in the terminal.", "tokens": [51314, 407, 286, 478, 3798, 456, 293, 286, 478, 1382, 281, 1322, 484, 1080, 3485, 281, 15529, 1491, 760, 257, 992, 295, 2577, 264, 8936, 1491, 5093, 82, 294, 264, 14709, 13, 51664], "temperature": 0.0, "avg_logprob": -0.22295904631661898, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.003759225830435753}, {"id": 222, "seek": 87800, "start": 878.0, "end": 891.0, "text": " And there was a lot of utilities and frameworks to make this possible and I kept I forget the exact of the wall that I was hitting but I kept hitting a wall and using one of these frameworks that was like just a limiter and this was a hard stop for me.", "tokens": [50364, 400, 456, 390, 257, 688, 295, 30482, 293, 29834, 281, 652, 341, 1944, 293, 286, 4305, 286, 2870, 264, 1900, 295, 264, 2929, 300, 286, 390, 8850, 457, 286, 4305, 8850, 257, 2929, 293, 1228, 472, 295, 613, 29834, 300, 390, 411, 445, 257, 2364, 1681, 293, 341, 390, 257, 1152, 1590, 337, 385, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15789798154669293, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.05860234797000885}, {"id": 223, "seek": 89100, "start": 891.0, "end": 904.0, "text": " And okay, like I need to progress I need to do something and I ended up asking hey like what are my options as the first I'm just asking is that when I was asking it more questions to tell learn more.", "tokens": [50364, 400, 1392, 11, 411, 286, 643, 281, 4205, 286, 643, 281, 360, 746, 293, 286, 4590, 493, 3365, 4177, 411, 437, 366, 452, 3956, 382, 264, 700, 286, 478, 445, 3365, 307, 300, 562, 286, 390, 3365, 309, 544, 1651, 281, 980, 1466, 544, 13, 51014], "temperature": 0.0, "avg_logprob": -0.26344492898058536, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.3141750395298004}, {"id": 224, "seek": 89100, "start": 904.0, "end": 909.0, "text": " And it's like oh I can write a markdown parser for you.", "tokens": [51014, 400, 309, 311, 411, 1954, 286, 393, 2464, 257, 1491, 5093, 21156, 260, 337, 291, 13, 51264], "temperature": 0.0, "avg_logprob": -0.26344492898058536, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.3141750395298004}, {"id": 225, "seek": 90900, "start": 909.0, "end": 918.0, "text": " I said cool go for it. And so this is the first time when it one shot its own markdown rendering.", "tokens": [50364, 286, 848, 1627, 352, 337, 309, 13, 400, 370, 341, 307, 264, 700, 565, 562, 309, 472, 3347, 1080, 1065, 1491, 5093, 22407, 13, 50814], "temperature": 0.0, "avg_logprob": -0.255374187376441, "compression_ratio": 1.5594059405940595, "no_speech_prob": 0.25320005416870117}, {"id": 226, "seek": 90900, "start": 918.0, "end": 922.0, "text": " And to be frank the variant of that is still used today that actually renders.", "tokens": [50814, 400, 281, 312, 10455, 264, 17501, 295, 300, 307, 920, 1143, 965, 300, 767, 6125, 433, 13, 51014], "temperature": 0.0, "avg_logprob": -0.255374187376441, "compression_ratio": 1.5594059405940595, "no_speech_prob": 0.25320005416870117}, {"id": 227, "seek": 90900, "start": 922.0, "end": 928.0, "text": " So if you're ever curious like if like Gemini CLI has written a significant amount of its own code.", "tokens": [51014, 407, 498, 291, 434, 1562, 6369, 411, 498, 411, 22894, 3812, 12855, 40, 575, 3720, 257, 4776, 2372, 295, 1080, 1065, 3089, 13, 51314], "temperature": 0.0, "avg_logprob": -0.255374187376441, "compression_ratio": 1.5594059405940595, "no_speech_prob": 0.25320005416870117}, {"id": 228, "seek": 90900, "start": 928.0, "end": 931.0, "text": " It's super significant amount so cool.", "tokens": [51314, 467, 311, 1687, 4776, 2372, 370, 1627, 13, 51464], "temperature": 0.0, "avg_logprob": -0.255374187376441, "compression_ratio": 1.5594059405940595, "no_speech_prob": 0.25320005416870117}, {"id": 229, "seek": 93100, "start": 931.0, "end": 960.0, "text": " Yeah, it's we doubt like I think the biggest thing that we think about on the team is with AI using AI these days it is so easy to 10X yourself which sounds crazy to say but 10Xing is easy these days 100Xing that's the hard part like that is where you really start getting into how can I parallelize my workflows to make sure my time that I'm investing in each of these things is best spent.", "tokens": [50364, 865, 11, 309, 311, 321, 6385, 411, 286, 519, 264, 3880, 551, 300, 321, 519, 466, 322, 264, 1469, 307, 365, 7318, 1228, 7318, 613, 1708, 309, 307, 370, 1858, 281, 1266, 55, 1803, 597, 3263, 3219, 281, 584, 457, 1266, 55, 278, 307, 1858, 613, 1708, 2319, 55, 278, 300, 311, 264, 1152, 644, 411, 300, 307, 689, 291, 534, 722, 1242, 666, 577, 393, 286, 8952, 1125, 452, 43461, 281, 652, 988, 452, 565, 300, 286, 478, 10978, 294, 1184, 295, 613, 721, 307, 1151, 4418, 13, 51814], "temperature": 0.0, "avg_logprob": -0.20031769044937625, "compression_ratio": 1.649789029535865, "no_speech_prob": 0.1939001828432083}, {"id": 230, "seek": 96000, "start": 960.0, "end": 977.0, "text": " Because models themselves that we talked about it there like there's so much that's left unsaid when asking a question they it still needs human feedback a lot of the times in order to be effective like things don't just get one shot of the right of course you see that in all the videos out there and like all the highlights.", "tokens": [50364, 1436, 5245, 2969, 300, 321, 2825, 466, 309, 456, 411, 456, 311, 370, 709, 300, 311, 1411, 2693, 17810, 562, 3365, 257, 1168, 436, 309, 920, 2203, 1952, 5824, 257, 688, 295, 264, 1413, 294, 1668, 281, 312, 4942, 411, 721, 500, 380, 445, 483, 472, 3347, 295, 264, 558, 295, 1164, 291, 536, 300, 294, 439, 264, 2145, 484, 456, 293, 411, 439, 264, 14254, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17502659949186805, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.24457484483718872}, {"id": 231, "seek": 96000, "start": 977.0, "end": 988.0, "text": " But one shoting rarely happens so how do you optimize your time to make it so that multi shot scenarios are the land super well they can really amplify what you do.", "tokens": [51214, 583, 472, 3347, 278, 13752, 2314, 370, 577, 360, 291, 19719, 428, 565, 281, 652, 309, 370, 300, 4825, 3347, 15077, 366, 264, 2117, 1687, 731, 436, 393, 534, 41174, 437, 291, 360, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17502659949186805, "compression_ratio": 1.7535714285714286, "no_speech_prob": 0.24457484483718872}, {"id": 232, "seek": 98800, "start": 988.0, "end": 1003.0, "text": " Another thing I'm curious about is are there any like methodologies you think about when using Gemini CLI like what's your mentality on how the tool operates is grounded in a lot of my developer background especially with AI.", "tokens": [50364, 3996, 551, 286, 478, 6369, 466, 307, 366, 456, 604, 411, 3170, 6204, 291, 519, 466, 562, 1228, 22894, 3812, 12855, 40, 411, 437, 311, 428, 21976, 322, 577, 264, 2290, 22577, 307, 23535, 294, 257, 688, 295, 452, 10754, 3678, 2318, 365, 7318, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12522143733744717, "compression_ratio": 1.4240506329113924, "no_speech_prob": 0.0013011277187615633}, {"id": 233, "seek": 100300, "start": 1003.0, "end": 1027.0, "text": " Which is I personally feel like there's so much that's left unsaid when people will do a prop so like one of the big things is this is context engineering it's kind of grounded in the same mindset which is when you ask a question to your AI of choice you're giving it as much information as you possibly can to enable it to derive the right answer.", "tokens": [50364, 3013, 307, 286, 5665, 841, 411, 456, 311, 370, 709, 300, 311, 1411, 2693, 17810, 562, 561, 486, 360, 257, 2365, 370, 411, 472, 295, 264, 955, 721, 307, 341, 307, 4319, 7043, 309, 311, 733, 295, 23535, 294, 264, 912, 12543, 597, 307, 562, 291, 1029, 257, 1168, 281, 428, 7318, 295, 3922, 291, 434, 2902, 309, 382, 709, 1589, 382, 291, 6264, 393, 281, 9528, 309, 281, 28446, 264, 558, 1867, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17278368045122194, "compression_ratio": 1.5890410958904109, "no_speech_prob": 0.5106940865516663}, {"id": 234, "seek": 102700, "start": 1027.0, "end": 1046.0, "text": " Now of course you're also leaning on its ability to like figure out more information and dig through your co base or whatever you're by doing at the time but that alone is really challenging spot because it can't know those offline conversations you have with your colleague or your friend it can't know.", "tokens": [50364, 823, 295, 1164, 291, 434, 611, 23390, 322, 1080, 3485, 281, 411, 2573, 484, 544, 1589, 293, 2528, 807, 428, 598, 3096, 420, 2035, 291, 434, 538, 884, 412, 264, 565, 457, 300, 3312, 307, 534, 7595, 4008, 570, 309, 393, 380, 458, 729, 21857, 7315, 291, 362, 365, 428, 13532, 420, 428, 1277, 309, 393, 380, 458, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16161170838371156, "compression_ratio": 1.5916230366492146, "no_speech_prob": 0.10943561047315598}, {"id": 235, "seek": 104600, "start": 1047.0, "end": 1065.0, "text": " Usually doesn't know your emails and your chat messages but it doesn't typically know that all those pieces of context really feed into building a coherent response so if you're currently writing tests and you ask to go ahead and implement something chances are you want to implement that in the form of tests.", "tokens": [50414, 11419, 1177, 380, 458, 428, 12524, 293, 428, 5081, 7897, 457, 309, 1177, 380, 5850, 458, 300, 439, 729, 3755, 295, 4319, 534, 3154, 666, 2390, 257, 36239, 4134, 370, 498, 291, 434, 4362, 3579, 6921, 293, 291, 1029, 281, 352, 2286, 293, 4445, 746, 10486, 366, 291, 528, 281, 4445, 300, 294, 264, 1254, 295, 6921, 13, 51314], "temperature": 0.0, "avg_logprob": -0.19170213514758694, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.5619468688964844}, {"id": 236, "seek": 106500, "start": 1065.0, "end": 1094.0, "text": " Your current workflow that methodology though it kind of rings true with every decision we make I think the thing I tell the team is that do what a person would do and don't take shortcuts so one thing I might be surprising to folks is we don't use embeddings like for instance for search we do not index your code base we do a gentick search which means how you as a person or use as a developer would dig through a piece of code we do the same thing.", "tokens": [50414, 2260, 2190, 20993, 300, 24850, 1673, 309, 733, 295, 11136, 2074, 365, 633, 3537, 321, 652, 286, 519, 264, 551, 286, 980, 264, 1469, 307, 300, 360, 437, 257, 954, 576, 360, 293, 500, 380, 747, 34620, 370, 472, 551, 286, 1062, 312, 8830, 281, 4024, 307, 321, 500, 380, 764, 12240, 29432, 411, 337, 5197, 337, 3164, 321, 360, 406, 8186, 428, 3089, 3096, 321, 360, 257, 16108, 618, 3164, 597, 1355, 577, 291, 382, 257, 954, 420, 764, 382, 257, 10754, 576, 2528, 807, 257, 2522, 295, 3089, 321, 360, 264, 912, 551, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1801897492071595, "compression_ratio": 1.765625, "no_speech_prob": 0.05947885662317276}, {"id": 237, "seek": 109500, "start": 1095.0, "end": 1121.0, "text": " We'll do fine we'll do we'll we'll grab our way through the code base we'll open files or read them will effectively run commands like find all references on your behalf if we need to to find out what's the next piece of the puzzle because in the end we're trying to provide the right context to the LM so that it's grounded in every single thing it does to come to a good a good result so I think you think it's probably it was probably the biggest one.", "tokens": [50364, 492, 603, 360, 2489, 321, 603, 360, 321, 603, 321, 603, 4444, 527, 636, 807, 264, 3089, 3096, 321, 603, 1269, 7098, 420, 1401, 552, 486, 8659, 1190, 16901, 411, 915, 439, 15400, 322, 428, 9490, 498, 321, 643, 281, 281, 915, 484, 437, 311, 264, 958, 2522, 295, 264, 12805, 570, 294, 264, 917, 321, 434, 1382, 281, 2893, 264, 558, 4319, 281, 264, 46529, 370, 300, 309, 311, 23535, 294, 633, 2167, 551, 309, 775, 281, 808, 281, 257, 665, 257, 665, 1874, 370, 286, 519, 291, 519, 309, 311, 1391, 309, 390, 1391, 264, 3880, 472, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14457903458521917, "compression_ratio": 1.700374531835206, "no_speech_prob": 0.01876557245850563}, {"id": 238, "seek": 112100, "start": 1122.0, "end": 1133.0, "text": " I think that kind of really resonates with us something that I think is really cool is like maybe I'll ask the CLI to do something and it'll try to do the thing but be like well I can't actually do this.", "tokens": [50414, 286, 519, 300, 733, 295, 534, 41051, 365, 505, 746, 300, 286, 519, 307, 534, 1627, 307, 411, 1310, 286, 603, 1029, 264, 12855, 40, 281, 360, 746, 293, 309, 603, 853, 281, 360, 264, 551, 457, 312, 411, 731, 286, 393, 380, 767, 360, 341, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06341470990862165, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.0604107603430748}, {"id": 239, "seek": 112100, "start": 1134.0, "end": 1145.0, "text": " But then you'll give me the steps that it needs to take in order to do it and be like are you okay with me going ahead and doing these steps to do the thing that I need to do to get to your original request which is very cool.", "tokens": [51014, 583, 550, 291, 603, 976, 385, 264, 4439, 300, 309, 2203, 281, 747, 294, 1668, 281, 360, 309, 293, 312, 411, 366, 291, 1392, 365, 385, 516, 2286, 293, 884, 613, 4439, 281, 360, 264, 551, 300, 286, 643, 281, 360, 281, 483, 281, 428, 3380, 5308, 597, 307, 588, 1627, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06341470990862165, "compression_ratio": 1.8297872340425532, "no_speech_prob": 0.0604107603430748}, {"id": 240, "seek": 114500, "start": 1146.0, "end": 1165.0, "text": " I love that's the that's such a huge unlock is the we call it self-healing it's ability to self-heal goes so far and like when it they'll try things like it has a good idea of what's on your box and to try to use all those things that when it can't do it it's so good at coming of other alternatives like I recall this one scenario.", "tokens": [50414, 286, 959, 300, 311, 264, 300, 311, 1270, 257, 2603, 11634, 307, 264, 321, 818, 309, 2698, 12, 675, 4270, 309, 311, 3485, 281, 2698, 12, 675, 304, 1709, 370, 1400, 293, 411, 562, 309, 436, 603, 853, 721, 411, 309, 575, 257, 665, 1558, 295, 437, 311, 322, 428, 2424, 293, 281, 853, 281, 764, 439, 729, 721, 300, 562, 309, 393, 380, 360, 309, 309, 311, 370, 665, 412, 1348, 295, 661, 20478, 411, 286, 9901, 341, 472, 9005, 13, 51364], "temperature": 0.0, "avg_logprob": -0.23738275572311046, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.042127497494220734}, {"id": 241, "seek": 116500, "start": 1165.0, "end": 1176.0, "text": " I was talking to our marketing folks and I was giving them a demo to show what it could do and then the first question was oh can I like can you give me a link to this and I'm like oh.", "tokens": [50364, 286, 390, 1417, 281, 527, 6370, 4024, 293, 286, 390, 2902, 552, 257, 10723, 281, 855, 437, 309, 727, 360, 293, 550, 264, 700, 1168, 390, 1954, 393, 286, 411, 393, 291, 976, 385, 257, 2113, 281, 341, 293, 286, 478, 411, 1954, 13, 50914], "temperature": 0.0, "avg_logprob": -0.17774573525229653, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.30258873105049133}, {"id": 242, "seek": 116500, "start": 1177.0, "end": 1187.0, "text": " It's we don't we don't have built in like deploy functionality like there's a cloud run and there's a cloud run extension actually being released very soon that will enable this just the folks know.", "tokens": [50964, 467, 311, 321, 500, 380, 321, 500, 380, 362, 3094, 294, 411, 7274, 14980, 411, 456, 311, 257, 4588, 1190, 293, 456, 311, 257, 4588, 1190, 10320, 767, 885, 4736, 588, 2321, 300, 486, 9528, 341, 445, 264, 4024, 458, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17774573525229653, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.30258873105049133}, {"id": 243, "seek": 118700, "start": 1188.0, "end": 1193.0, "text": " But we don't have that built in and so they're asking how can you do this I'm like okay well let me just ask.", "tokens": [50414, 583, 321, 500, 380, 362, 300, 3094, 294, 293, 370, 436, 434, 3365, 577, 393, 291, 360, 341, 286, 478, 411, 1392, 731, 718, 385, 445, 1029, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14770782404932484, "compression_ratio": 1.7526132404181185, "no_speech_prob": 0.20563624799251556}, {"id": 244, "seek": 118700, "start": 1194.0, "end": 1216.0, "text": " And what it did is it ended up like creating a GitHub repository and GitHub repositories have a thing called GitHub pages which allows you to host static content and then it pushed this content to it and it gave me a link and I gave it I'm like I had never even considered that my question was how do I give this person like it but all the other pieces together to kind of make that a reality.", "tokens": [50714, 400, 437, 309, 630, 307, 309, 4590, 493, 411, 4084, 257, 23331, 25841, 293, 23331, 22283, 2083, 362, 257, 551, 1219, 23331, 7183, 597, 4045, 291, 281, 3975, 13437, 2701, 293, 550, 309, 9152, 341, 2701, 281, 309, 293, 309, 2729, 385, 257, 2113, 293, 286, 2729, 309, 286, 478, 411, 286, 632, 1128, 754, 4888, 300, 452, 1168, 390, 577, 360, 286, 976, 341, 954, 411, 309, 457, 439, 264, 661, 3755, 1214, 281, 733, 295, 652, 300, 257, 4103, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14770782404932484, "compression_ratio": 1.7526132404181185, "no_speech_prob": 0.20563624799251556}, {"id": 245, "seek": 121600, "start": 1216.0, "end": 1230.0, "text": " Like the scrap scrapiest developer yeah I'm almost thinking of the use case where I'm like hey Gemini CLI how can I like tell my mom that I'm still alive like up there and it'll like ask me maybe you should do that but hear the steps that I would take like are you okay.", "tokens": [50364, 1743, 264, 23138, 23138, 6495, 10754, 1338, 286, 478, 1920, 1953, 295, 264, 764, 1389, 689, 286, 478, 411, 4177, 22894, 3812, 12855, 40, 577, 393, 286, 411, 980, 452, 1225, 300, 286, 478, 920, 5465, 411, 493, 456, 293, 309, 603, 411, 1029, 385, 1310, 291, 820, 360, 300, 457, 1568, 264, 4439, 300, 286, 576, 747, 411, 366, 291, 1392, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2715842064390791, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.04047558456659317}, {"id": 246, "seek": 121600, "start": 1231.0, "end": 1233.0, "text": " I think about the text messages and whatever.", "tokens": [51114, 286, 519, 466, 264, 2487, 7897, 293, 2035, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2715842064390791, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.04047558456659317}, {"id": 247, "seek": 121600, "start": 1234.0, "end": 1238.0, "text": " Totally yeah and my mom watches these so mom I'm still alive.", "tokens": [51264, 22837, 1338, 293, 452, 1225, 17062, 613, 370, 1225, 286, 478, 920, 5465, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2715842064390791, "compression_ratio": 1.695067264573991, "no_speech_prob": 0.04047558456659317}, {"id": 248, "seek": 123800, "start": 1238.0, "end": 1247.0, "text": " Well and also a moment ago you mentioned something about like an upcoming feature on the roadmap.", "tokens": [50364, 1042, 293, 611, 257, 1623, 2057, 291, 2835, 746, 466, 411, 364, 11500, 4111, 322, 264, 35738, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1465309429168701, "compression_ratio": 1.6131386861313868, "no_speech_prob": 0.015820469707250595}, {"id": 249, "seek": 123800, "start": 1248.0, "end": 1255.0, "text": " So what else like what else is coming up on the roadmap like are there any features coming up that you're most excited for.", "tokens": [50864, 407, 437, 1646, 411, 437, 1646, 307, 1348, 493, 322, 264, 35738, 411, 366, 456, 604, 4122, 1348, 493, 300, 291, 434, 881, 2919, 337, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1465309429168701, "compression_ratio": 1.6131386861313868, "no_speech_prob": 0.015820469707250595}, {"id": 250, "seek": 125500, "start": 1255.0, "end": 1272.0, "text": " Because we view Gemini CLI as being a lot more than just developers and because we've seen internally it's unlocking every profession whether you're a marketer or a financial to of course the software developer it kind of tax that hits all those buttons to make it so we can work with every one of these professions.", "tokens": [50364, 1436, 321, 1910, 22894, 3812, 12855, 40, 382, 885, 257, 688, 544, 813, 445, 8849, 293, 570, 321, 600, 1612, 19501, 309, 311, 49620, 633, 7032, 1968, 291, 434, 257, 2142, 260, 420, 257, 4669, 281, 295, 1164, 264, 4722, 10754, 309, 733, 295, 3366, 300, 8664, 439, 729, 9905, 281, 652, 309, 370, 321, 393, 589, 365, 633, 472, 295, 613, 38129, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16799774845089532, "compression_ratio": 1.7251655629139073, "no_speech_prob": 0.47878602147102356}, {"id": 251, "seek": 125500, "start": 1273.0, "end": 1284.0, "text": " We're really doubling down an extensibility so you can heavily extend Gemini CLI and this is not just MCP servers this is literally you can install an extension which is a bundle of it could be available.", "tokens": [51264, 492, 434, 534, 33651, 760, 364, 1279, 694, 2841, 370, 291, 393, 10950, 10101, 22894, 3812, 12855, 40, 293, 341, 307, 406, 445, 8797, 47, 15909, 341, 307, 3736, 291, 393, 3625, 364, 10320, 597, 307, 257, 24438, 295, 309, 727, 312, 2435, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16799774845089532, "compression_ratio": 1.7251655629139073, "no_speech_prob": 0.47878602147102356}, {"id": 252, "seek": 128500, "start": 1285.0, "end": 1295.0, "text": " MCP servers specific instructions specific commands lots of different things to drive a different so this is the cloud run one that I had mentioned earlier they have an extension.", "tokens": [50364, 8797, 47, 15909, 2685, 9415, 2685, 16901, 3195, 295, 819, 721, 281, 3332, 257, 819, 370, 341, 307, 264, 4588, 1190, 472, 300, 286, 632, 2835, 3071, 436, 362, 364, 10320, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17484968002528362, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.003759806277230382}, {"id": 253, "seek": 128500, "start": 1296.0, "end": 1308.0, "text": " And this extension can be Gemini extensions install and then even passing cloud run and it's seamless installation but enables you to really curate the experience to your like your preferences.", "tokens": [50914, 400, 341, 10320, 393, 312, 22894, 3812, 25129, 3625, 293, 550, 754, 8437, 4588, 1190, 293, 309, 311, 28677, 13260, 457, 17077, 291, 281, 534, 1262, 473, 264, 1752, 281, 428, 411, 428, 21910, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17484968002528362, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.003759806277230382}, {"id": 254, "seek": 130800, "start": 1308.0, "end": 1328.0, "text": " So for instance if you are like a go developer and you want to make sure your environment is super go friendly like you'll install the right MCP servers to make that happen right or if you are it's a content generator maybe you'll hook it up to all your various socials maybe you'll create a generative media APIs you'll also hook that up to it.", "tokens": [50364, 407, 337, 5197, 498, 291, 366, 411, 257, 352, 10754, 293, 291, 528, 281, 652, 988, 428, 2823, 307, 1687, 352, 9208, 411, 291, 603, 3625, 264, 558, 8797, 47, 15909, 281, 652, 300, 1051, 558, 420, 498, 291, 366, 309, 311, 257, 2701, 19265, 1310, 291, 603, 6328, 309, 493, 281, 439, 428, 3683, 2093, 82, 1310, 291, 603, 1884, 257, 1337, 1166, 3021, 21445, 291, 603, 611, 6328, 300, 493, 281, 309, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1198719724824157, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.18121078610420227}, {"id": 255, "seek": 132800, "start": 1328.0, "end": 1353.0, "text": " So being able to turn these on and off is something that's super important to us because we know that there's a lot of use cases and so the big so that's the biggest feature that we're going to be talking about soon which is how to build these extensions how to install them manage them with the intention of making this super seamless for people where people can spin up their own registries for they want to we're going to have eventually going to have like a centralized registry for all of our extensions.", "tokens": [50364, 407, 885, 1075, 281, 1261, 613, 322, 293, 766, 307, 746, 300, 311, 1687, 1021, 281, 505, 570, 321, 458, 300, 456, 311, 257, 688, 295, 764, 3331, 293, 370, 264, 955, 370, 300, 311, 264, 3880, 4111, 300, 321, 434, 516, 281, 312, 1417, 466, 2321, 597, 307, 577, 281, 1322, 613, 25129, 577, 281, 3625, 552, 3067, 552, 365, 264, 7789, 295, 1455, 341, 1687, 28677, 337, 561, 689, 561, 393, 6060, 493, 641, 1065, 11376, 2244, 337, 436, 528, 281, 321, 434, 516, 281, 362, 4728, 516, 281, 362, 411, 257, 32395, 36468, 337, 439, 295, 527, 25129, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11874112543070091, "compression_ratio": 1.8713235294117647, "no_speech_prob": 0.5417033433914185}, {"id": 256, "seek": 135300, "start": 1354.0, "end": 1363.0, "text": " But the extension ecosystem is going to be the big one I can't wait to see what people build like we have a number of them coming out from Google from from cloud.", "tokens": [50414, 583, 264, 10320, 11311, 307, 516, 281, 312, 264, 955, 472, 286, 393, 380, 1699, 281, 536, 437, 561, 1322, 411, 321, 362, 257, 1230, 295, 552, 1348, 484, 490, 3329, 490, 490, 4588, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12968789485462925, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.28538215160369873}, {"id": 257, "seek": 135300, "start": 1365.0, "end": 1370.0, "text": " And just in general to really hook Gemini CLI into everything in a really seamless way.", "tokens": [50964, 400, 445, 294, 2674, 281, 534, 6328, 22894, 3812, 12855, 40, 666, 1203, 294, 257, 534, 28677, 636, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12968789485462925, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.28538215160369873}, {"id": 258, "seek": 135300, "start": 1371.0, "end": 1381.0, "text": " Yeah Taylor I just wanted to thank you so much for sharing your insights with us in our audience of agent builders it's been a fascinating look behind the curtain of Gemini CLI and you've shared so much with us so so I want to thank you.", "tokens": [51264, 865, 12060, 286, 445, 1415, 281, 1309, 291, 370, 709, 337, 5414, 428, 14310, 365, 505, 294, 527, 4034, 295, 9461, 36281, 309, 311, 668, 257, 10343, 574, 2261, 264, 26789, 295, 22894, 3812, 12855, 40, 293, 291, 600, 5507, 370, 709, 365, 505, 370, 370, 286, 528, 281, 1309, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12968789485462925, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.28538215160369873}, {"id": 259, "seek": 138100, "start": 1382.0, "end": 1407.0, "text": " Oh thanks for having me this has been an amazing conversation like I love I love diving in and especially love just sharing stories and if you haven't checked this out checks out on GitHub you'll find us all the amazing like Gemini Google dash Gemini slash Gemini CLI and get up and then of course you can look for us in socials as well to get those weekly updates that we push out regularly.", "tokens": [50414, 876, 3231, 337, 1419, 385, 341, 575, 668, 364, 2243, 3761, 411, 286, 959, 286, 959, 20241, 294, 293, 2318, 959, 445, 5414, 3676, 293, 498, 291, 2378, 380, 10033, 341, 484, 13834, 484, 322, 23331, 291, 603, 915, 505, 439, 264, 2243, 411, 22894, 3812, 3329, 8240, 22894, 3812, 17330, 22894, 3812, 12855, 40, 293, 483, 493, 293, 550, 295, 1164, 291, 393, 574, 337, 505, 294, 2093, 82, 382, 731, 281, 483, 729, 12460, 9205, 300, 321, 2944, 484, 11672, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15139323267443427, "compression_ratio": 1.6540084388185654, "no_speech_prob": 0.01612197980284691}, {"id": 260, "seek": 140700, "start": 1408.0, "end": 1411.0, "text": " Oh yeah good plug yeah thank you so much Taylor this is so fun.", "tokens": [50414, 876, 1338, 665, 5452, 1338, 1309, 291, 370, 709, 12060, 341, 307, 370, 1019, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1543097178141276, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.02982158586382866}, {"id": 261, "seek": 140700, "start": 1412.0, "end": 1413.0, "text": " Yeah thanks so much Taylor.", "tokens": [50614, 865, 3231, 370, 709, 12060, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1543097178141276, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.02982158586382866}, {"id": 262, "seek": 140700, "start": 1414.0, "end": 1421.0, "text": " And that's our show for today thank you for joining us for this deep dive into the Gemini CLI we highly recommend you try it out for yourself.", "tokens": [50714, 400, 300, 311, 527, 855, 337, 965, 1309, 291, 337, 5549, 505, 337, 341, 2452, 9192, 666, 264, 22894, 3812, 12855, 40, 321, 5405, 2748, 291, 853, 309, 484, 337, 1803, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1543097178141276, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.02982158586382866}, {"id": 263, "seek": 140700, "start": 1422.0, "end": 1433.0, "text": " And if you enjoyed this episode of the agent factory check us out next time where we'll continue diving into the world of AI agents until then I'm Emmett Mirage and I'm Molly Pettit.", "tokens": [51114, 400, 498, 291, 4626, 341, 3500, 295, 264, 9461, 9265, 1520, 505, 484, 958, 565, 689, 321, 603, 2354, 20241, 666, 264, 1002, 295, 7318, 12554, 1826, 550, 286, 478, 28237, 3093, 9421, 609, 293, 286, 478, 26665, 430, 3093, 270, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1543097178141276, "compression_ratio": 1.641732283464567, "no_speech_prob": 0.02982158586382866}, {"id": 264, "seek": 143300, "start": 1433.0, "end": 1435.0, "text": " Power it down.", "tokens": [50414, 7086, 309, 760, 13, 50464], "temperature": 0.0, "avg_logprob": -0.4073152542114258, "compression_ratio": 0.6363636363636364, "no_speech_prob": 0.39038974046707153}], "language": "en"}